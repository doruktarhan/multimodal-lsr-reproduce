{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "dense_embs = load_dataset(\"lsr42/mscoco-blip-dense\", data_files={\"img_emb\": \"img_embs.parquet\", \"text_emb\": \"text_embs.parquet\"}, keep_in_memory=True).with_format(\"numpy\")\n",
    "meta_data = json.load(open(hf_hub_download(\n",
    "    repo_id=\"lsr42/mscoco-blip-dense\", repo_type=\"dataset\", filename=\"dataset_meta.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['img_emb', 'text_emb'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123287, 256)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs['img_emb']['emb'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(616767, 256)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs['text_emb']['emb'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'emb'],\n",
       "    num_rows: 123287\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs['img_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'dataset'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coco'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data['dataset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filepath', 'sentids', 'filename', 'imgid', 'split', 'sentences', 'cocoid'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data['images'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': 'val2014',\n",
       " 'sentids': [681330, 686718, 688839, 693159, 693204],\n",
       " 'filename': 'COCO_val2014_000000522418.jpg',\n",
       " 'imgid': 1,\n",
       " 'split': 'restval',\n",
       " 'sentences': [{'tokens': ['a',\n",
       "    'woman',\n",
       "    'wearing',\n",
       "    'a',\n",
       "    'net',\n",
       "    'on',\n",
       "    'her',\n",
       "    'head',\n",
       "    'cutting',\n",
       "    'a',\n",
       "    'cake'],\n",
       "   'raw': 'A woman wearing a net on her head cutting a cake. ',\n",
       "   'imgid': 1,\n",
       "   'sentid': 681330},\n",
       "  {'tokens': ['a', 'woman', 'cutting', 'a', 'large', 'white', 'sheet', 'cake'],\n",
       "   'raw': 'A woman cutting a large white sheet cake.',\n",
       "   'imgid': 1,\n",
       "   'sentid': 686718},\n",
       "  {'tokens': ['a',\n",
       "    'woman',\n",
       "    'wearing',\n",
       "    'a',\n",
       "    'hair',\n",
       "    'net',\n",
       "    'cutting',\n",
       "    'a',\n",
       "    'large',\n",
       "    'sheet',\n",
       "    'cake'],\n",
       "   'raw': 'A woman wearing a hair net cutting a large sheet cake.',\n",
       "   'imgid': 1,\n",
       "   'sentid': 688839},\n",
       "  {'tokens': ['there',\n",
       "    'is',\n",
       "    'a',\n",
       "    'woman',\n",
       "    'that',\n",
       "    'is',\n",
       "    'cutting',\n",
       "    'a',\n",
       "    'white',\n",
       "    'cake'],\n",
       "   'raw': 'there is a woman that is cutting a white cake',\n",
       "   'imgid': 1,\n",
       "   'sentid': 693159},\n",
       "  {'tokens': ['a',\n",
       "    'woman',\n",
       "    'marking',\n",
       "    'a',\n",
       "    'cake',\n",
       "    'with',\n",
       "    'the',\n",
       "    'back',\n",
       "    'of',\n",
       "    'a',\n",
       "    'chefs',\n",
       "    'knife'],\n",
       "   'raw': \"A woman marking a cake with the back of a chef's knife. \",\n",
       "   'imgid': 1,\n",
       "   'sentid': 693204}],\n",
       " 'cocoid': 522418}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data['images'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding shape: (256,)\n",
      "Number of text embeddings found: 5\n",
      "Text embedding shapes: [(256,), (256,), (256,), (256,), (256,)]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text embeddings for fast lookup, normalizing keys to strings\n",
    "text_emb_lookup = {str(item['id']): item['emb'] for item in dense_embs['text_emb']}  # Ensure all keys are strings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding shape: (256,)\n",
      "Number of text embeddings found: 5\n",
      "Text embedding shapes: [(256,), (256,), (256,), (256,), (256,)]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve image embedding\n",
    "imgid = meta_data['images'][0]['imgid']\n",
    "example_img_embed = dense_embs['img_emb'][imgid]['emb']\n",
    "\n",
    "# Retrieve text embeddings for the corresponding sentence IDs\n",
    "sentids = meta_data['images'][0]['sentids']\n",
    "text_embeddings = [text_emb_lookup[str(sentid)] for sentid in sentids if str(sentid) in text_emb_lookup]\n",
    "\n",
    "# Output\n",
    "print(f\"Image embedding shape: {example_img_embed.shape}\")\n",
    "print(f\"Number of text embeddings found: {len(text_embeddings)}\")\n",
    "print(f\"Text embedding shapes: {[emb.shape for emb in text_embeddings]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding 0 cosine similarity: 0.43064552545547485\n",
      "Text embedding 1 cosine similarity: 0.42722970247268677\n",
      "Text embedding 2 cosine similarity: 0.33973366022109985\n",
      "Text embedding 3 cosine similarity: 0.4240642786026001\n",
      "Text embedding 4 cosine similarity: 0.4385703504085541\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\"\"\"\n",
    "for i in range(len(text_embeddings)):\n",
    "    cosine_sim_matrix = cosine_similarity(example_img_embed.reshape(-1,1), text_embeddings[i].reshape(-1,1))\n",
    "    print(f\"Text embedding {i} cosine similarity: {cosine_sim_matrix}\")\n",
    "\n",
    "\"\"\"\n",
    "for i in range(len(text_embeddings)):\n",
    "    pairwise_cosine_sim = np.matmul(example_img_embed, text_embeddings[i].T)\n",
    "    print(f\"Text embedding {i} cosine similarity: {pairwise_cosine_sim}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "# Define paths\n",
    "path_to_dataset_folder = '/Users/doruktarhan/Desktop/MSCOCO_Dataset' #dataset images folder path\n",
    "# Use glob to find all image files across subdirectories\n",
    "image_files = glob(f'{path_to_dataset_folder}/**/*.jpg', recursive=True)\n",
    "all_image_paths = {os.path.basename(file): file for file in image_files}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load metadata\n",
    "meta_data_path = 'data/dataset_coco.json' #metadata file path\n",
    "with open(meta_data_path, 'r') as f:\n",
    "    meta_data_kaggle = json.load(f)\n",
    "\n",
    "# Display images and metadata for the first 5 instances\n",
    "for idx, metadata_item in enumerate(meta_data_kaggle['images'][:5]):\n",
    "    filename = metadata_item[\"filename\"]  \n",
    "    captions = [sentence[\"raw\"] for sentence in metadata_item[\"sentences\"]]  # Captions list\n",
    "    \n",
    "    # Use the glob-based lookup dictionary to find the image path\n",
    "    image_path = all_image_paths.get(filename)\n",
    "    if not image_path:\n",
    "        print(f\"Image not found: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Display the metadata and image\n",
    "    print(f\"Metadata for instance {idx + 1}:\")\n",
    "    print(f\"Filename: {filename}\")\n",
    "    print(f\"Captions: {captions}\")\n",
    "\n",
    "    # Plot the image and captions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Captions: {captions[0]}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_kaggle['images'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert metadata to a pandas DataFrame\n",
    "metadata_images = meta_data_kaggle['images']  # Extract the 'images' key\n",
    "df = pd.DataFrame(metadata_images)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Check unique splits and their counts\n",
    "split_counts = df['split'].value_counts()\n",
    "\n",
    "print(\"\\nUnique Splits:\")\n",
    "print(split_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLIP Encoder Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForImageTextRetrieval\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "model_name = \"Salesforce/blip-itm-large-coco\" #model card for image text matching\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForImageTextRetrieval.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "#load metadata\n",
    "meta_data_path = 'data/dataset_coco.json'\n",
    "with open(meta_data_path, 'r') as f:\n",
    "    meta_data_kaggle = json.load(f)\n",
    "\n",
    "\n",
    "#image path \n",
    "path_to_dataset_folder = '/Users/doruktarhan/Desktop/MSCOCO_Dataset' #dataset images folder path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Define paths\n",
    "path_to_dataset_folder = '/Users/doruktarhan/Desktop/MSCOCO_Dataset' #dataset images folder path\n",
    "# Use glob to find all image files across subdirectories\n",
    "image_files = glob(f'{path_to_dataset_folder}/**/*.jpg', recursive=True)\n",
    "all_image_paths = {os.path.basename(file): file for file in image_files}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load metadata\n",
    "meta_data_path = 'data/dataset_coco.json' #metadata file path\n",
    "with open(meta_data_path, 'r') as f:\n",
    "    meta_data_kaggle = json.load(f)\n",
    "\n",
    "# Display images and metadata for the first 5 instances\n",
    "for idx, metadata_item in enumerate(meta_data_kaggle['images'][:1]):\n",
    "    filename = metadata_item[\"filename\"]  # e.g., '1000092795.jpg'\n",
    "    captions = [sentence[\"raw\"] for sentence in metadata_item[\"sentences\"]]  # Captions list\n",
    "    \n",
    "    # Use the glob-based lookup dictionary to find the image path\n",
    "    image_path = all_image_paths.get(filename)\n",
    "    if not image_path:\n",
    "        print(f\"Image not found: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Display the metadata and image\n",
    "    print(f\"Metadata for instance {idx + 1}:\")\n",
    "    print(f\"Filename: {filename}\")\n",
    "    print(f\"Captions: {captions}\")\n",
    "\n",
    "    # Plot the image and captions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Captions: {captions[0]}\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "for caption in captions:\n",
    "    inputs = processor(images=image, text=caption, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "    print(f'Caption: {caption}')\n",
    "    print(outputs['itm_score'])\n",
    "    probabilities = softmax(outputs['itm_score'], dim=1)\n",
    "    print(probabilities)\n",
    "    print(outputs['question_embeds'].shape)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = 'Man riding on a motorcycle with blue helmet'\n",
    "inputs = processor(images=image, text=caption, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "print(f'Caption: {caption}')\n",
    "print(f\"Output tensor itm matching score: {outputs['itm_score']}\")\n",
    "probabilities = softmax(outputs['itm_score'], dim=1)\n",
    "print(f\"Probabilities after softmax: {probabilities}\")\n",
    "\n",
    "print(f\"Question embedding shape: {outputs['question_embeds'].shape}\")\n",
    "print(f\"Image embedding shape: {outputs['last_hidden_state'].shape}\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize\n",
    "\n",
    "proj_text_embedding = normalize(model.text_proj(outputs.question_embeds[:,0,:]))\n",
    "proj_image_embedding = normalize(model.vision_proj(outputs.last_hidden_state[:,0,:]))\n",
    "\n",
    "\n",
    "print(f\"Image projection shape: {proj_image_embedding.shape}\")\n",
    "print(f\"Text projection shape: {proj_text_embedding.shape}\")\n",
    "\n",
    "# Compute pairwise cosine similarity (matrix)\n",
    "pairwise_cosine_sim = torch.matmul(proj_image_embedding, proj_text_embedding.T)\n",
    "\n",
    "print(f\"Pairwise cosine similarity:\\n{pairwise_cosine_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
