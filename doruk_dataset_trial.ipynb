{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "dense_embs = load_dataset(\"doruktarhan6/flickr30k-blip-dense\", data_files={\"img_emb\": \"img_embs.parquet\", \"text_emb\": \"text_embs.parquet\"}, keep_in_memory=True).with_format(\"numpy\")\n",
    "meta_data = json.load(open(hf_hub_download(\n",
    "    repo_id=\"doruktarhan6/flickr30k-blip-dense\", repo_type=\"dataset\", filename=\"dataset_meta.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "dense_embs_lsr = load_dataset(\"lsr42/flickr30k-blip-dense\", data_files={\"img_emb\": \"img_embs.parquet\", \"text_emb\": \"text_embs.parquet\"}, keep_in_memory=True).with_format(\"numpy\")\n",
    "meta_data_lsr = json.load(open(hf_hub_download(\n",
    "    repo_id=\"lsr42/flickr30k-blip-dense\", repo_type=\"dataset\", filename=\"dataset_meta.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['img_emb', 'text_emb'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'dataset'])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'emb'])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs['img_emb'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['images', 'dataset']),\n",
       " 'flickr30k',\n",
       " dict_keys(['sentids', 'imgid', 'sentences', 'split', 'filename']))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data.keys(), meta_data['dataset'], meta_data['images'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, numpy.int64, numpy.str_, int)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(meta_data['images'][0]['imgid']), type(dense_embs['img_emb'][0]['id']), type(dense_embs_lsr['img_emb'][0]['id']),type(meta_data_lsr['images'][0]['imgid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentids</th>\n",
       "      <th>imgid</th>\n",
       "      <th>sentences</th>\n",
       "      <th>split</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[600, 601, 602, 603, 604]</td>\n",
       "      <td>120</td>\n",
       "      <td>[{'tokens': ['a', 'man', 'in', 'a', 'black', '...</td>\n",
       "      <td>train</td>\n",
       "      <td>103625356.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[605, 606, 607, 608, 609]</td>\n",
       "      <td>121</td>\n",
       "      <td>[{'tokens': ['a', 'caucasian', 'young', 'woman...</td>\n",
       "      <td>train</td>\n",
       "      <td>103631543.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[610, 611, 612, 613, 614]</td>\n",
       "      <td>122</td>\n",
       "      <td>[{'tokens': ['a', 'woman', 'in', 'a', 'blue', ...</td>\n",
       "      <td>train</td>\n",
       "      <td>1039360778.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[615, 616, 617, 618, 619]</td>\n",
       "      <td>123</td>\n",
       "      <td>[{'tokens': ['someone', 'is', 'standing', 'on'...</td>\n",
       "      <td>train</td>\n",
       "      <td>103953336.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[620, 621, 622, 623, 624]</td>\n",
       "      <td>124</td>\n",
       "      <td>[{'tokens': ['a', 'bride', 'in', 'a', 'light',...</td>\n",
       "      <td>test</td>\n",
       "      <td>1039637574.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[625, 626, 627, 628, 629]</td>\n",
       "      <td>125</td>\n",
       "      <td>[{'tokens': ['young', 'white', 'boy', 'on', 'a...</td>\n",
       "      <td>train</td>\n",
       "      <td>10404007.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[630, 631, 632, 633, 634]</td>\n",
       "      <td>126</td>\n",
       "      <td>[{'tokens': ['a', 'man', 'is', 'cooking', 'wha...</td>\n",
       "      <td>train</td>\n",
       "      <td>1040426962.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[635, 636, 637, 638, 639]</td>\n",
       "      <td>127</td>\n",
       "      <td>[{'tokens': ['three', 'people', 'are', 'on', '...</td>\n",
       "      <td>train</td>\n",
       "      <td>104136873.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[640, 641, 642, 643, 644]</td>\n",
       "      <td>128</td>\n",
       "      <td>[{'tokens': ['a', 'group', 'of', 'tourists', '...</td>\n",
       "      <td>train</td>\n",
       "      <td>104180524.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[645, 646, 647, 648, 649]</td>\n",
       "      <td>129</td>\n",
       "      <td>[{'tokens': ['a', 'young', 'boy', 'wearing', '...</td>\n",
       "      <td>train</td>\n",
       "      <td>1042020065.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sentids  imgid  \\\n",
       "120  [600, 601, 602, 603, 604]    120   \n",
       "121  [605, 606, 607, 608, 609]    121   \n",
       "122  [610, 611, 612, 613, 614]    122   \n",
       "123  [615, 616, 617, 618, 619]    123   \n",
       "124  [620, 621, 622, 623, 624]    124   \n",
       "125  [625, 626, 627, 628, 629]    125   \n",
       "126  [630, 631, 632, 633, 634]    126   \n",
       "127  [635, 636, 637, 638, 639]    127   \n",
       "128  [640, 641, 642, 643, 644]    128   \n",
       "129  [645, 646, 647, 648, 649]    129   \n",
       "\n",
       "                                             sentences  split        filename  \n",
       "120  [{'tokens': ['a', 'man', 'in', 'a', 'black', '...  train   103625356.jpg  \n",
       "121  [{'tokens': ['a', 'caucasian', 'young', 'woman...  train   103631543.jpg  \n",
       "122  [{'tokens': ['a', 'woman', 'in', 'a', 'blue', ...  train  1039360778.jpg  \n",
       "123  [{'tokens': ['someone', 'is', 'standing', 'on'...  train   103953336.jpg  \n",
       "124  [{'tokens': ['a', 'bride', 'in', 'a', 'light',...   test  1039637574.jpg  \n",
       "125  [{'tokens': ['young', 'white', 'boy', 'on', 'a...  train    10404007.jpg  \n",
       "126  [{'tokens': ['a', 'man', 'is', 'cooking', 'wha...  train  1040426962.jpg  \n",
       "127  [{'tokens': ['three', 'people', 'are', 'on', '...  train   104136873.jpg  \n",
       "128  [{'tokens': ['a', 'group', 'of', 'tourists', '...  train   104180524.jpg  \n",
       "129  [{'tokens': ['a', 'young', 'boy', 'wearing', '...  train  1042020065.jpg  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metadata_df = pd.DataFrame(meta_data['images'])\n",
    "metadata_df[120:130]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentids': [0, 1, 2, 3, 4],\n",
       " 'imgid': 0,\n",
       " 'sentences': [{'tokens': ['two',\n",
       "    'young',\n",
       "    'guys',\n",
       "    'with',\n",
       "    'shaggy',\n",
       "    'hair',\n",
       "    'look',\n",
       "    'at',\n",
       "    'their',\n",
       "    'hands',\n",
       "    'while',\n",
       "    'hanging',\n",
       "    'out',\n",
       "    'in',\n",
       "    'the',\n",
       "    'yard'],\n",
       "   'raw': 'Two young guys with shaggy hair look at their hands while hanging out in the yard.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 0},\n",
       "  {'tokens': ['two',\n",
       "    'young',\n",
       "    'white',\n",
       "    'males',\n",
       "    'are',\n",
       "    'outside',\n",
       "    'near',\n",
       "    'many',\n",
       "    'bushes'],\n",
       "   'raw': 'Two young, White males are outside near many bushes.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 1},\n",
       "  {'tokens': ['two',\n",
       "    'men',\n",
       "    'in',\n",
       "    'green',\n",
       "    'shirts',\n",
       "    'are',\n",
       "    'standing',\n",
       "    'in',\n",
       "    'a',\n",
       "    'yard'],\n",
       "   'raw': 'Two men in green shirts are standing in a yard.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 2},\n",
       "  {'tokens': ['a',\n",
       "    'man',\n",
       "    'in',\n",
       "    'a',\n",
       "    'blue',\n",
       "    'shirt',\n",
       "    'standing',\n",
       "    'in',\n",
       "    'a',\n",
       "    'garden'],\n",
       "   'raw': 'A man in a blue shirt standing in a garden.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 3},\n",
       "  {'tokens': ['two', 'friends', 'enjoy', 'time', 'spent', 'together'],\n",
       "   'raw': 'Two friends enjoy time spent together.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 4}],\n",
       " 'split': 'train',\n",
       " 'filename': '1000092795.jpg'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dense_embs['img_emb']['id'] to a set for faster lookup\n",
    "dense_img_ids = set(int(item['id']) for item in dense_embs['img_emb'])\n",
    "\n",
    "# Filter images in metadata\n",
    "filtered_images = [\n",
    "    image for image in meta_data['images']\n",
    "    if image['imgid'] in dense_img_ids\n",
    "]\n",
    "\n",
    "# Create the new metadata dictionary with filtered images\n",
    "filtered_metadata = {\n",
    "    \"dataset\": meta_data['dataset'],\n",
    "    \"images\": filtered_images\n",
    "}\n",
    "\n",
    "filtered_metadata['images'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30402, 31014)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_metadata['images']), len(meta_data['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered metadata saved to dataset_meta.json\n"
     ]
    }
   ],
   "source": [
    "# Save filtered metadata back to JSON\n",
    "filtered_metadata_path = \"dataset_meta.json\"\n",
    "with open(filtered_metadata_path, \"w\") as file:\n",
    "    json.dump(filtered_metadata, file, indent=2)\n",
    "\n",
    "print(f\"Filtered metadata saved to {filtered_metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentids</th>\n",
       "      <th>imgid</th>\n",
       "      <th>sentences</th>\n",
       "      <th>split</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[600, 601, 602, 603, 604]</td>\n",
       "      <td>120</td>\n",
       "      <td>[{'tokens': ['a', 'man', 'in', 'a', 'black', '...</td>\n",
       "      <td>train</td>\n",
       "      <td>103625356.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[605, 606, 607, 608, 609]</td>\n",
       "      <td>121</td>\n",
       "      <td>[{'tokens': ['a', 'caucasian', 'young', 'woman...</td>\n",
       "      <td>train</td>\n",
       "      <td>103631543.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[610, 611, 612, 613, 614]</td>\n",
       "      <td>122</td>\n",
       "      <td>[{'tokens': ['a', 'woman', 'in', 'a', 'blue', ...</td>\n",
       "      <td>train</td>\n",
       "      <td>1039360778.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[615, 616, 617, 618, 619]</td>\n",
       "      <td>123</td>\n",
       "      <td>[{'tokens': ['someone', 'is', 'standing', 'on'...</td>\n",
       "      <td>train</td>\n",
       "      <td>103953336.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[620, 621, 622, 623, 624]</td>\n",
       "      <td>124</td>\n",
       "      <td>[{'tokens': ['a', 'bride', 'in', 'a', 'light',...</td>\n",
       "      <td>test</td>\n",
       "      <td>1039637574.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[625, 626, 627, 628, 629]</td>\n",
       "      <td>125</td>\n",
       "      <td>[{'tokens': ['young', 'white', 'boy', 'on', 'a...</td>\n",
       "      <td>train</td>\n",
       "      <td>10404007.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[630, 631, 632, 633, 634]</td>\n",
       "      <td>126</td>\n",
       "      <td>[{'tokens': ['a', 'man', 'is', 'cooking', 'wha...</td>\n",
       "      <td>train</td>\n",
       "      <td>1040426962.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[635, 636, 637, 638, 639]</td>\n",
       "      <td>127</td>\n",
       "      <td>[{'tokens': ['three', 'people', 'are', 'on', '...</td>\n",
       "      <td>train</td>\n",
       "      <td>104136873.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[640, 641, 642, 643, 644]</td>\n",
       "      <td>128</td>\n",
       "      <td>[{'tokens': ['a', 'group', 'of', 'tourists', '...</td>\n",
       "      <td>train</td>\n",
       "      <td>104180524.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[650, 651, 652, 653, 654]</td>\n",
       "      <td>130</td>\n",
       "      <td>[{'tokens': ['a', 'older', 'man', 'holds', 'an...</td>\n",
       "      <td>train</td>\n",
       "      <td>1042359076.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sentids  imgid  \\\n",
       "120  [600, 601, 602, 603, 604]    120   \n",
       "121  [605, 606, 607, 608, 609]    121   \n",
       "122  [610, 611, 612, 613, 614]    122   \n",
       "123  [615, 616, 617, 618, 619]    123   \n",
       "124  [620, 621, 622, 623, 624]    124   \n",
       "125  [625, 626, 627, 628, 629]    125   \n",
       "126  [630, 631, 632, 633, 634]    126   \n",
       "127  [635, 636, 637, 638, 639]    127   \n",
       "128  [640, 641, 642, 643, 644]    128   \n",
       "129  [650, 651, 652, 653, 654]    130   \n",
       "\n",
       "                                             sentences  split        filename  \n",
       "120  [{'tokens': ['a', 'man', 'in', 'a', 'black', '...  train   103625356.jpg  \n",
       "121  [{'tokens': ['a', 'caucasian', 'young', 'woman...  train   103631543.jpg  \n",
       "122  [{'tokens': ['a', 'woman', 'in', 'a', 'blue', ...  train  1039360778.jpg  \n",
       "123  [{'tokens': ['someone', 'is', 'standing', 'on'...  train   103953336.jpg  \n",
       "124  [{'tokens': ['a', 'bride', 'in', 'a', 'light',...   test  1039637574.jpg  \n",
       "125  [{'tokens': ['young', 'white', 'boy', 'on', 'a...  train    10404007.jpg  \n",
       "126  [{'tokens': ['a', 'man', 'is', 'cooking', 'wha...  train  1040426962.jpg  \n",
       "127  [{'tokens': ['three', 'people', 'are', 'on', '...  train   104136873.jpg  \n",
       "128  [{'tokens': ['a', 'group', 'of', 'tourists', '...  train   104180524.jpg  \n",
       "129  [{'tokens': ['a', 'older', 'man', 'holds', 'an...  train  1042359076.jpg  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(filtered_metadata['images'])[120:130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([120, 121, 122, 123, 124, 125, 126, 127, 128, 130])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs['img_emb'][:]['id'][120:130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metadata_df = pd.DataFrame(meta_data['images'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID '125' exists in metadata: False\n"
     ]
    }
   ],
   "source": [
    "exists_in_meta = any(image['id'] == 125 for image in dense_embs['img_emb'])\n",
    "print(f\"ID '125' exists in metadata: {exists_in_meta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.022006208077073097, 0.09969383478164673, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.09531231224536896, 0.04221922904253006, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.039520345628261566, 0.10081849247217178, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.09637868404388428, 0.12081631273031235, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.016611870378255844, 0.10799410194158554, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                emb\n",
       "0   0  [-0.022006208077073097, 0.09969383478164673, 0...\n",
       "1   1  [0.09531231224536896, 0.04221922904253006, -0....\n",
       "2   2  [-0.039520345628261566, 0.10081849247217178, 0...\n",
       "3   3  [0.09637868404388428, 0.12081631273031235, -0....\n",
       "4   4  [0.016611870378255844, 0.10799410194158554, -0..."
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs['img_emb'].to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31014, 256)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs['img_emb']['emb'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155070, 256)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs['text_emb']['emb'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'emb'],\n",
       "    num_rows: 31014\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embs['img_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'dataset'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coco'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data['dataset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filepath', 'sentids', 'filename', 'imgid', 'split', 'sentences', 'cocoid'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data['images'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': 'val2014',\n",
       " 'sentids': [681330, 686718, 688839, 693159, 693204],\n",
       " 'filename': 'COCO_val2014_000000522418.jpg',\n",
       " 'imgid': 1,\n",
       " 'split': 'restval',\n",
       " 'sentences': [{'tokens': ['a',\n",
       "    'woman',\n",
       "    'wearing',\n",
       "    'a',\n",
       "    'net',\n",
       "    'on',\n",
       "    'her',\n",
       "    'head',\n",
       "    'cutting',\n",
       "    'a',\n",
       "    'cake'],\n",
       "   'raw': 'A woman wearing a net on her head cutting a cake. ',\n",
       "   'imgid': 1,\n",
       "   'sentid': 681330},\n",
       "  {'tokens': ['a', 'woman', 'cutting', 'a', 'large', 'white', 'sheet', 'cake'],\n",
       "   'raw': 'A woman cutting a large white sheet cake.',\n",
       "   'imgid': 1,\n",
       "   'sentid': 686718},\n",
       "  {'tokens': ['a',\n",
       "    'woman',\n",
       "    'wearing',\n",
       "    'a',\n",
       "    'hair',\n",
       "    'net',\n",
       "    'cutting',\n",
       "    'a',\n",
       "    'large',\n",
       "    'sheet',\n",
       "    'cake'],\n",
       "   'raw': 'A woman wearing a hair net cutting a large sheet cake.',\n",
       "   'imgid': 1,\n",
       "   'sentid': 688839},\n",
       "  {'tokens': ['there',\n",
       "    'is',\n",
       "    'a',\n",
       "    'woman',\n",
       "    'that',\n",
       "    'is',\n",
       "    'cutting',\n",
       "    'a',\n",
       "    'white',\n",
       "    'cake'],\n",
       "   'raw': 'there is a woman that is cutting a white cake',\n",
       "   'imgid': 1,\n",
       "   'sentid': 693159},\n",
       "  {'tokens': ['a',\n",
       "    'woman',\n",
       "    'marking',\n",
       "    'a',\n",
       "    'cake',\n",
       "    'with',\n",
       "    'the',\n",
       "    'back',\n",
       "    'of',\n",
       "    'a',\n",
       "    'chefs',\n",
       "    'knife'],\n",
       "   'raw': \"A woman marking a cake with the back of a chef's knife. \",\n",
       "   'imgid': 1,\n",
       "   'sentid': 693204}],\n",
       " 'cocoid': 522418}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data['images'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding shape: (256,)\n",
      "Number of text embeddings found: 5\n",
      "Text embedding shapes: [(256,), (256,), (256,), (256,), (256,)]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text embeddings for fast lookup, normalizing keys to strings\n",
    "text_emb_lookup = {str(item['id']): item['emb'] for item in dense_embs['text_emb']}  # Ensure all keys are strings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding shape: (256,)\n",
      "Number of text embeddings found: 5\n",
      "Text embedding shapes: [(256,), (256,), (256,), (256,), (256,)]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve image embedding\n",
    "imgid = meta_data['images'][0]['imgid']\n",
    "example_img_embed = dense_embs['img_emb'][imgid]['emb']\n",
    "\n",
    "# Retrieve text embeddings for the corresponding sentence IDs\n",
    "sentids = meta_data['images'][0]['sentids']\n",
    "text_embeddings = [text_emb_lookup[str(sentid)] for sentid in sentids if str(sentid) in text_emb_lookup]\n",
    "\n",
    "# Output\n",
    "print(f\"Image embedding shape: {example_img_embed.shape}\")\n",
    "print(f\"Number of text embeddings found: {len(text_embeddings)}\")\n",
    "print(f\"Text embedding shapes: {[emb.shape for emb in text_embeddings]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding 0 cosine similarity: 0.43064552545547485\n",
      "Text embedding 1 cosine similarity: 0.42722970247268677\n",
      "Text embedding 2 cosine similarity: 0.33973366022109985\n",
      "Text embedding 3 cosine similarity: 0.4240642786026001\n",
      "Text embedding 4 cosine similarity: 0.4385703504085541\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\"\"\"\n",
    "for i in range(len(text_embeddings)):\n",
    "    cosine_sim_matrix = cosine_similarity(example_img_embed.reshape(-1,1), text_embeddings[i].reshape(-1,1))\n",
    "    print(f\"Text embedding {i} cosine similarity: {cosine_sim_matrix}\")\n",
    "\n",
    "\"\"\"\n",
    "for i in range(len(text_embeddings)):\n",
    "    pairwise_cosine_sim = np.matmul(example_img_embed, text_embeddings[i].T)\n",
    "    print(f\"Text embedding {i} cosine similarity: {pairwise_cosine_sim}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "# Define paths\n",
    "path_to_dataset_folder = '/Users/doruktarhan/Desktop/MSCOCO_Dataset' #dataset images folder path\n",
    "# Use glob to find all image files across subdirectories\n",
    "image_files = glob(f'{path_to_dataset_folder}/**/*.jpg', recursive=True)\n",
    "all_image_paths = {os.path.basename(file): file for file in image_files}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load metadata\n",
    "meta_data_path = 'data/dataset_coco.json' #metadata file path\n",
    "with open(meta_data_path, 'r') as f:\n",
    "    meta_data_kaggle = json.load(f)\n",
    "\n",
    "# Display images and metadata for the first 5 instances\n",
    "for idx, metadata_item in enumerate(meta_data_kaggle['images'][:5]):\n",
    "    filename = metadata_item[\"filename\"]  \n",
    "    captions = [sentence[\"raw\"] for sentence in metadata_item[\"sentences\"]]  # Captions list\n",
    "    \n",
    "    # Use the glob-based lookup dictionary to find the image path\n",
    "    image_path = all_image_paths.get(filename)\n",
    "    if not image_path:\n",
    "        print(f\"Image not found: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Display the metadata and image\n",
    "    print(f\"Metadata for instance {idx + 1}:\")\n",
    "    print(f\"Filename: {filename}\")\n",
    "    print(f\"Captions: {captions}\")\n",
    "\n",
    "    # Plot the image and captions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Captions: {captions[0]}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_kaggle['images'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert metadata to a pandas DataFrame\n",
    "metadata_images = meta_data_kaggle['images']  # Extract the 'images' key\n",
    "df = pd.DataFrame(metadata_images)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Check unique splits and their counts\n",
    "split_counts = df['split'].value_counts()\n",
    "\n",
    "print(\"\\nUnique Splits:\")\n",
    "print(split_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLIP Encoder Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForImageTextRetrieval\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "model_name = \"Salesforce/blip-itm-large-coco\" #model card for image text matching\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForImageTextRetrieval.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "#load metadata\n",
    "meta_data_path = 'data/dataset_coco.json'\n",
    "with open(meta_data_path, 'r') as f:\n",
    "    meta_data_kaggle = json.load(f)\n",
    "\n",
    "\n",
    "#image path \n",
    "path_to_dataset_folder = '/Users/doruktarhan/Desktop/MSCOCO_Dataset' #dataset images folder path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Define paths\n",
    "path_to_dataset_folder = '/Users/doruktarhan/Desktop/MSCOCO_Dataset' #dataset images folder path\n",
    "# Use glob to find all image files across subdirectories\n",
    "image_files = glob(f'{path_to_dataset_folder}/**/*.jpg', recursive=True)\n",
    "all_image_paths = {os.path.basename(file): file for file in image_files}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load metadata\n",
    "meta_data_path = 'data/dataset_coco.json' #metadata file path\n",
    "with open(meta_data_path, 'r') as f:\n",
    "    meta_data_kaggle = json.load(f)\n",
    "\n",
    "# Display images and metadata for the first 5 instances\n",
    "for idx, metadata_item in enumerate(meta_data_kaggle['images'][:1]):\n",
    "    filename = metadata_item[\"filename\"]  # e.g., '1000092795.jpg'\n",
    "    captions = [sentence[\"raw\"] for sentence in metadata_item[\"sentences\"]]  # Captions list\n",
    "    \n",
    "    # Use the glob-based lookup dictionary to find the image path\n",
    "    image_path = all_image_paths.get(filename)\n",
    "    if not image_path:\n",
    "        print(f\"Image not found: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Display the metadata and image\n",
    "    print(f\"Metadata for instance {idx + 1}:\")\n",
    "    print(f\"Filename: {filename}\")\n",
    "    print(f\"Captions: {captions}\")\n",
    "\n",
    "    # Plot the image and captions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Captions: {captions[0]}\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "for caption in captions:\n",
    "    inputs = processor(images=image, text=caption, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "    print(f'Caption: {caption}')\n",
    "    print(outputs['itm_score'])\n",
    "    probabilities = softmax(outputs['itm_score'], dim=1)\n",
    "    print(probabilities)\n",
    "    print(outputs['question_embeds'].shape)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = 'Man riding on a motorcycle with blue helmet'\n",
    "inputs = processor(images=image, text=caption, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "print(f'Caption: {caption}')\n",
    "print(f\"Output tensor itm matching score: {outputs['itm_score']}\")\n",
    "probabilities = softmax(outputs['itm_score'], dim=1)\n",
    "print(f\"Probabilities after softmax: {probabilities}\")\n",
    "\n",
    "print(f\"Question embedding shape: {outputs['question_embeds'].shape}\")\n",
    "print(f\"Image embedding shape: {outputs['last_hidden_state'].shape}\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize\n",
    "\n",
    "proj_text_embedding = normalize(model.text_proj(outputs.question_embeds[:,0,:]))\n",
    "proj_image_embedding = normalize(model.vision_proj(outputs.last_hidden_state[:,0,:]))\n",
    "\n",
    "\n",
    "print(f\"Image projection shape: {proj_image_embedding.shape}\")\n",
    "print(f\"Text projection shape: {proj_text_embedding.shape}\")\n",
    "\n",
    "# Compute pairwise cosine similarity (matrix)\n",
    "pairwise_cosine_sim = torch.matmul(proj_image_embedding, proj_text_embedding.T)\n",
    "\n",
    "print(f\"Pairwise cosine similarity:\\n{pairwise_cosine_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities for each image-text pair:\n",
      "Pair 1: 0.4717683494091034\n",
      "Pair 2: 0.4680973291397095\n",
      "Pair 3: 0.4207531213760376\n",
      "Pair 4: 0.5063284635543823\n",
      "Pair 5: 0.4703652560710907\n",
      "Pair 6: 0.4047545790672302\n",
      "Pair 7: 0.46468424797058105\n",
      "Pair 8: 0.46485379338264465\n",
      "Pair 9: 0.4473761022090912\n",
      "Pair 10: 0.5034165978431702\n",
      "Pair 11: 0.42352989315986633\n",
      "Pair 12: 0.5207198262214661\n",
      "Pair 13: 0.4432687759399414\n",
      "Pair 14: 0.44502633810043335\n",
      "Pair 15: 0.5233748555183411\n",
      "Pair 16: 0.44414687156677246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p9/056tygps4hv_9gn5wdt55xwh0000gn/T/ipykernel_78812/1152185754.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the saved embeddings\n",
    "file_path = \"test_embeddings/blip_test/test_batch.pt\"\n",
    "embeddings = torch.load(file_path)\n",
    "\n",
    "# Extract the embeddings\n",
    "image_embeds = embeddings[\"image_embeds\"]  # (batch_size, embedding_dim)\n",
    "text_embeds = embeddings[\"text_embeds\"]    # (batch_size, embedding_dim)\n",
    "\n",
    "# Normalize the embeddings\n",
    "image_embeds = torch.nn.functional.normalize(image_embeds, p=2, dim=1)  # Normalize along embedding dimension\n",
    "text_embeds = torch.nn.functional.normalize(text_embeds, p=2, dim=1)\n",
    "\n",
    "# Compute pairwise cosine similarity for each image-text pair\n",
    "cosine_similarities = torch.sum(image_embeds * text_embeds, dim=1)\n",
    "\n",
    "# Print the results\n",
    "print(\"Cosine similarities for each image-text pair:\")\n",
    "for i, sim in enumerate(cosine_similarities):\n",
    "    print(f\"Pair {i+1}: {sim.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0 and Text 0 Cosine Similarity: 0.4687182905649778\n",
      "Image 1 and Text 1 Cosine Similarity: 0.4822275800169071\n",
      "Image 2 and Text 2 Cosine Similarity: 0.44469575617662155\n",
      "Image 3 and Text 3 Cosine Similarity: 0.4633115597567263\n",
      "Image 4 and Text 4 Cosine Similarity: 0.4925434587492448\n",
      "Image 5 and Text 5 Cosine Similarity: 0.5880840729854441\n",
      "Image 6 and Text 6 Cosine Similarity: 0.5194537997895264\n",
      "Image 7 and Text 7 Cosine Similarity: 0.46692864103672294\n",
      "Image 8 and Text 8 Cosine Similarity: 0.5903741106656454\n",
      "Image 9 and Text 9 Cosine Similarity: 0.4781363231533651\n",
      "Image 10 and Text 10 Cosine Similarity: 0.4350797959041108\n",
      "Image 11 and Text 11 Cosine Similarity: 0.43959263238461244\n",
      "Image 12 and Text 12 Cosine Similarity: 0.43109933663207517\n",
      "Image 13 and Text 13 Cosine Similarity: 0.45771729806122613\n",
      "Image 14 and Text 14 Cosine Similarity: 0.44786657768916066\n",
      "Image 15 and Text 15 Cosine Similarity: 0.46102534364631664\n",
      "Image 16 and Text 16 Cosine Similarity: 0.4925554589395006\n",
      "Image 17 and Text 17 Cosine Similarity: 0.4837812128089\n",
      "Image 18 and Text 18 Cosine Similarity: 0.42512346834197656\n",
      "Image 19 and Text 19 Cosine Similarity: 0.5212530709235345\n",
      "Image 20 and Text 20 Cosine Similarity: 0.48199891340714285\n",
      "Image 21 and Text 21 Cosine Similarity: 0.504358908537022\n",
      "Image 22 and Text 22 Cosine Similarity: 0.48828202411454136\n",
      "Image 23 and Text 23 Cosine Similarity: 0.472046101791575\n",
      "Image 24 and Text 24 Cosine Similarity: 0.44315673609471967\n",
      "Image 25 and Text 25 Cosine Similarity: 0.5376687622692419\n",
      "Image 26 and Text 26 Cosine Similarity: 0.5039784910307068\n",
      "Image 27 and Text 27 Cosine Similarity: 0.4785532796753504\n",
      "Image 28 and Text 28 Cosine Similarity: 0.5323357346891774\n",
      "Image 29 and Text 29 Cosine Similarity: 0.5112277706350926\n",
      "Image 30 and Text 30 Cosine Similarity: 0.4946591832976309\n",
      "Image 31 and Text 31 Cosine Similarity: 0.48552650259830954\n",
      "Image 32 and Text 32 Cosine Similarity: 0.4931863674657157\n",
      "Image 33 and Text 33 Cosine Similarity: 0.47017417465264105\n",
      "Image 34 and Text 34 Cosine Similarity: 0.5125180638702901\n",
      "Image 35 and Text 35 Cosine Similarity: 0.4956500814127047\n",
      "Image 36 and Text 36 Cosine Similarity: 0.5091185634752132\n",
      "Image 37 and Text 37 Cosine Similarity: 0.46922364266599637\n",
      "Image 38 and Text 38 Cosine Similarity: 0.550800359466726\n",
      "Image 39 and Text 39 Cosine Similarity: 0.49343942685058373\n",
      "Image 40 and Text 40 Cosine Similarity: 0.4680826871303795\n",
      "Image 41 and Text 41 Cosine Similarity: 0.491492553110477\n",
      "Image 42 and Text 42 Cosine Similarity: 0.47018730993943797\n",
      "Image 43 and Text 43 Cosine Similarity: 0.47496470471526275\n",
      "Image 44 and Text 44 Cosine Similarity: 0.46984212480796933\n",
      "Image 45 and Text 45 Cosine Similarity: 0.5071593416758741\n",
      "Image 46 and Text 46 Cosine Similarity: 0.5121962631628142\n",
      "Image 47 and Text 47 Cosine Similarity: 0.4426080951110799\n",
      "Image 48 and Text 48 Cosine Similarity: 0.4934625797967344\n",
      "Image 49 and Text 49 Cosine Similarity: 0.5314044006900346\n",
      "Image 50 and Text 50 Cosine Similarity: 0.4722098150972955\n",
      "Image 51 and Text 51 Cosine Similarity: 0.4656496755781969\n",
      "Image 52 and Text 52 Cosine Similarity: 0.5152204693362259\n",
      "Image 53 and Text 53 Cosine Similarity: 0.4458239199249791\n",
      "Image 54 and Text 54 Cosine Similarity: 0.4809653953099211\n",
      "Image 55 and Text 55 Cosine Similarity: 0.4391556778353379\n",
      "Image 56 and Text 56 Cosine Similarity: 0.46335312832283876\n",
      "Image 57 and Text 57 Cosine Similarity: 0.4093496441866153\n",
      "Image 58 and Text 58 Cosine Similarity: 0.41788725092505125\n",
      "Image 59 and Text 59 Cosine Similarity: 0.4360903228482798\n",
      "Image 60 and Text 60 Cosine Similarity: 0.4551243590209572\n",
      "Image 61 and Text 61 Cosine Similarity: 0.4856093588380203\n",
      "Image 62 and Text 62 Cosine Similarity: 0.42628187914416665\n",
      "Image 63 and Text 63 Cosine Similarity: 0.44857265003100777\n",
      "Image 64 and Text 64 Cosine Similarity: 0.5232249976606751\n",
      "Image 65 and Text 65 Cosine Similarity: 0.528184432154052\n",
      "Image 66 and Text 66 Cosine Similarity: 0.5402244037217631\n",
      "Image 67 and Text 67 Cosine Similarity: 0.473073454606908\n",
      "Image 68 and Text 68 Cosine Similarity: 0.5137308509878378\n",
      "Image 69 and Text 69 Cosine Similarity: 0.4736283123713149\n",
      "Image 70 and Text 70 Cosine Similarity: 0.41726191460883877\n",
      "Image 71 and Text 71 Cosine Similarity: 0.43758180815893233\n",
      "Image 72 and Text 72 Cosine Similarity: 0.39426492515747197\n",
      "Image 73 and Text 73 Cosine Similarity: 0.5288505022164065\n",
      "Image 74 and Text 74 Cosine Similarity: 0.3965965323645543\n",
      "Image 75 and Text 75 Cosine Similarity: 0.4857155186194752\n",
      "Image 76 and Text 76 Cosine Similarity: 0.5141257243364903\n",
      "Image 77 and Text 77 Cosine Similarity: 0.5143284151010646\n",
      "Image 78 and Text 78 Cosine Similarity: 0.503421709422075\n",
      "Image 79 and Text 79 Cosine Similarity: 0.5290101294199876\n",
      "Image 80 and Text 80 Cosine Similarity: 0.4856268199804561\n",
      "Image 81 and Text 81 Cosine Similarity: 0.4331891756407305\n",
      "Image 82 and Text 82 Cosine Similarity: 0.4697317489995767\n",
      "Image 83 and Text 83 Cosine Similarity: 0.4789724787814299\n",
      "Image 84 and Text 84 Cosine Similarity: 0.5198900890711834\n",
      "Image 85 and Text 85 Cosine Similarity: 0.4461958582233475\n",
      "Image 86 and Text 86 Cosine Similarity: 0.4542542081517374\n",
      "Image 87 and Text 87 Cosine Similarity: 0.47839192733604585\n",
      "Image 88 and Text 88 Cosine Similarity: 0.39824011628527844\n",
      "Image 89 and Text 89 Cosine Similarity: 0.4726551554860619\n",
      "Image 90 and Text 90 Cosine Similarity: 0.4652483119737889\n",
      "Image 91 and Text 91 Cosine Similarity: 0.5400808666372219\n",
      "Image 92 and Text 92 Cosine Similarity: 0.43598390099259016\n",
      "Image 93 and Text 93 Cosine Similarity: 0.48579182098464596\n",
      "Image 94 and Text 94 Cosine Similarity: 0.4714096730583394\n",
      "Image 95 and Text 95 Cosine Similarity: 0.4847510379370306\n",
      "Image 96 and Text 96 Cosine Similarity: 0.44383008127795304\n",
      "Image 97 and Text 97 Cosine Similarity: 0.46414878851579316\n",
      "Image 98 and Text 98 Cosine Similarity: 0.43777822038949876\n",
      "Image 99 and Text 99 Cosine Similarity: 0.4990094599863971\n",
      "Image 100 and Text 100 Cosine Similarity: 0.4199897057545774\n",
      "Image 101 and Text 101 Cosine Similarity: 0.4580866209519037\n",
      "Image 102 and Text 102 Cosine Similarity: 0.49563658349484674\n",
      "Image 103 and Text 103 Cosine Similarity: 0.49075474143534387\n",
      "Image 104 and Text 104 Cosine Similarity: 0.46740262631981155\n",
      "Image 105 and Text 105 Cosine Similarity: 0.45040020727999536\n",
      "Image 106 and Text 106 Cosine Similarity: 0.4839428692765387\n",
      "Image 107 and Text 107 Cosine Similarity: 0.4915185684356277\n",
      "Image 108 and Text 108 Cosine Similarity: 0.49184995225380584\n",
      "Image 109 and Text 109 Cosine Similarity: 0.48937921451011623\n",
      "Image 110 and Text 110 Cosine Similarity: 0.5514557587935641\n",
      "Image 111 and Text 111 Cosine Similarity: 0.494575192634592\n",
      "Image 112 and Text 112 Cosine Similarity: 0.529883538949696\n",
      "Image 113 and Text 113 Cosine Similarity: 0.49147430093806943\n",
      "Image 114 and Text 114 Cosine Similarity: 0.5055840260655601\n",
      "Image 115 and Text 115 Cosine Similarity: 0.4864662877166301\n",
      "Image 116 and Text 116 Cosine Similarity: 0.5277326749417246\n",
      "Image 117 and Text 117 Cosine Similarity: 0.4984535508418482\n",
      "Image 118 and Text 118 Cosine Similarity: 0.44531512203660367\n",
      "Image 119 and Text 119 Cosine Similarity: 0.4978283901657232\n",
      "Image 120 and Text 120 Cosine Similarity: 0.4153559535791232\n",
      "Image 121 and Text 121 Cosine Similarity: 0.41529968705388337\n",
      "Image 122 and Text 122 Cosine Similarity: 0.41498887159841985\n",
      "Image 123 and Text 123 Cosine Similarity: 0.541466357325218\n",
      "Image 124 and Text 124 Cosine Similarity: 0.5215175699331317\n",
      "Image 125 and Text 125 Cosine Similarity: 0.3899185819942205\n",
      "Image 126 and Text 126 Cosine Similarity: 0.4444963827126556\n",
      "Image 127 and Text 127 Cosine Similarity: 0.4500034233228894\n",
      "Image 128 and Text 128 Cosine Similarity: 0.4930729291459276\n",
      "Image 129 and Text 129 Cosine Similarity: 0.5101459798129184\n",
      "Image 130 and Text 130 Cosine Similarity: 0.5922002143903835\n",
      "Image 131 and Text 131 Cosine Similarity: 0.5360052952089426\n",
      "Image 132 and Text 132 Cosine Similarity: 0.6020675550087624\n",
      "Image 133 and Text 133 Cosine Similarity: 0.6016587459042618\n",
      "Image 134 and Text 134 Cosine Similarity: 0.607712410176933\n",
      "Image 135 and Text 135 Cosine Similarity: 0.46467440882934924\n",
      "Image 136 and Text 136 Cosine Similarity: 0.547254983174742\n",
      "Image 137 and Text 137 Cosine Similarity: 0.4832824066396491\n",
      "Image 138 and Text 138 Cosine Similarity: 0.4648942037194801\n",
      "Image 139 and Text 139 Cosine Similarity: 0.46947859884940113\n",
      "Image 140 and Text 140 Cosine Similarity: 0.4742510490089677\n",
      "Image 141 and Text 141 Cosine Similarity: 0.48770128464770374\n",
      "Image 142 and Text 142 Cosine Similarity: 0.5649495159452771\n",
      "Image 143 and Text 143 Cosine Similarity: 0.5187028795316052\n",
      "Image 144 and Text 144 Cosine Similarity: 0.5444174206043282\n",
      "Image 145 and Text 145 Cosine Similarity: 0.5470360862297972\n",
      "Image 146 and Text 146 Cosine Similarity: 0.48821849846953574\n",
      "Image 147 and Text 147 Cosine Similarity: 0.5371727758737145\n",
      "Image 148 and Text 148 Cosine Similarity: 0.49859561142124614\n",
      "Image 149 and Text 149 Cosine Similarity: 0.5403411300109843\n",
      "Image 150 and Text 150 Cosine Similarity: 0.4737446939758316\n",
      "Image 151 and Text 151 Cosine Similarity: 0.47375697690962004\n",
      "Image 152 and Text 152 Cosine Similarity: 0.4181654703105033\n",
      "Image 153 and Text 153 Cosine Similarity: 0.4837263282422299\n",
      "Image 154 and Text 154 Cosine Similarity: 0.4667818005538686\n",
      "Image 155 and Text 155 Cosine Similarity: 0.5019091527264643\n",
      "Image 156 and Text 156 Cosine Similarity: 0.4659831623013968\n",
      "Image 157 and Text 157 Cosine Similarity: 0.41334050449215115\n",
      "Image 158 and Text 158 Cosine Similarity: 0.5401293387305858\n",
      "Image 159 and Text 159 Cosine Similarity: 0.4956383848595857\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Paths to the Parquet files\n",
    "img_embs_path = \"embeddings_small/img_embs.parquet\"\n",
    "text_embs_path = \"embeddings_small/text_embs.parquet\"\n",
    "\n",
    "# Load Parquet files\n",
    "img_embs_df = pd.read_parquet(img_embs_path)\n",
    "text_embs_df = pd.read_parquet(text_embs_path)\n",
    "\n",
    "# Ensure one-to-one correspondence\n",
    "assert len(img_embs_df) == len(text_embs_df), \"Mismatch in image and text embeddings!\"\n",
    "\n",
    "# Extract embeddings and convert them to tensors\n",
    "img_embs = torch.tensor(np.vstack(img_embs_df['embedding'].values))\n",
    "text_embs = torch.tensor(np.vstack(text_embs_df['embedding'].values))\n",
    "\n",
    "# Normalize embeddings for cosine similarity (optional, depends on the model output)\n",
    "img_embs_normalized = torch.nn.functional.normalize(img_embs, dim=1)\n",
    "text_embs_normalized = torch.nn.functional.normalize(text_embs, dim=1)\n",
    "\n",
    "# Compute pairwise cosine similarity using matrix multiplication\n",
    "cosine_similarities = torch.matmul(img_embs_normalized, text_embs_normalized.T)\n",
    "\n",
    "# Extract diagonal elements for one-to-one correspondence cosine similarities\n",
    "one_to_one_cosine_similarities = cosine_similarities.diag()\n",
    "\n",
    "# Print the cosine similarities\n",
    "for idx, sim in enumerate(one_to_one_cosine_similarities):\n",
    "    print(f\"Image {idx} and Text {idx} Cosine Similarity: {sim.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_embs_df['index'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filepath': 'val2014', 'sentids': [105783, 106152, 111315, 116598, 122208], 'filename': 'COCO_val2014_000000017778.jpg', 'imgid': 19779, 'split': 'restval', 'sentences': [{'tokens': ['a', 'hummingbird', 'flying', 'above', 'a', 'bunch', 'of', 'small', 'red', 'and', 'white', 'flowers'], 'raw': 'a hummingbird flying above a bunch of small red and white flowers', 'imgid': 19779, 'sentid': 105783}, {'tokens': ['the', 'gerania', 'are', 'flowering', 'with', 'pink', 'white', 'and', 'red', 'florets'], 'raw': 'The gerania are flowering with pink, white, and red florets.', 'imgid': 19779, 'sentid': 106152}, {'tokens': ['a', 'humming', 'bird', 'above', 'a', 'fushia', 'flower', 'among', 'green', 'plants'], 'raw': 'A humming bird above a fushia flower among green plants. ', 'imgid': 19779, 'sentid': 111315}, {'tokens': ['an', 'insect', 'perches', 'on', 'a', 'flower', 'amidst', 'green', 'leaves'], 'raw': 'An insect perches on a flower amidst green leaves.', 'imgid': 19779, 'sentid': 116598}, {'tokens': ['a', 'close', 'up', 'of', 'a', 'small', 'bird', 'on', 'a', 'small', 'flower'], 'raw': 'a close up of a small bird on a small flower ', 'imgid': 19779, 'sentid': 122208}], 'cocoid': 17778}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load metadata\n",
    "meta_data_path = 'meta_data/dataset_coco.json'  # Replace with your metadata path\n",
    "image_name = 'COCO_val2014_000000017778.jpg'\n",
    "\n",
    "with open(meta_data_path, 'r') as file:\n",
    "    metadata = json.load(file)\n",
    "\n",
    "# Find the image\n",
    "for image_data in metadata['images']:\n",
    "    if image_data['filename'] == image_name:\n",
    "        print(image_data)\n",
    "        break\n",
    "else:\n",
    "    print(f\"Image '{image_name}' not found in metadata.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load the image\n",
    "image_path = ('/Users/doruktarhan/Desktop/MSCOCO_trial_images_small/val2014/COCO_val2014_000000017953.jpg')\n",
    "image = Image.open(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = \"embeddings_small/\"\n",
    "\n",
    "dense_embs = pd.read_parquet(os.path.join(directory, \"img_embs.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dense_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pd.read_parquet(\"/Users/doruktarhan/Desktop/LSR_Encoder_Results/flickr30k-blip-dense_old/text_embs.parquet\")\n",
    "\n",
    "\n",
    "\n",
    "# Save it back without the index\n",
    "df.to_parquet(\"/Users/doruktarhan/Desktop/LSR_Encoder_Results/flickr30k-blip-dense/text_embs.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0025575943291187286, 0.0009181360946968198...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.07348638027906418, 0.038891274482011795, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.03266528621315956, 0.11210073530673981, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.013137374073266983, -0.000507828954141587, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.05367390066385269, -0.05757979676127434, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                emb\n",
       "0   0  [-0.0025575943291187286, 0.0009181360946968198...\n",
       "1   1  [-0.07348638027906418, 0.038891274482011795, 0...\n",
       "2   2  [0.03266528621315956, 0.11210073530673981, -0....\n",
       "3   3  [0.013137374073266983, -0.000507828954141587, ...\n",
       "4   4  [0.05367390066385269, -0.05757979676127434, -0..."
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.022006208077073097, 0.09969383478164673, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.09531231224536896, 0.04221922904253006, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.039520345628261566, 0.10081849247217178, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.09637868404388428, 0.12081631273031235, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.016611870378255844, 0.10799410194158554, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                emb\n",
       "0   0  [-0.022006208077073097, 0.09969383478164673, 0...\n",
       "1   1  [0.09531231224536896, 0.04221922904253006, -0....\n",
       "2   2  [-0.039520345628261566, 0.10081849247217178, 0...\n",
       "3   3  [0.09637868404388428, 0.12081631273031235, -0....\n",
       "4   4  [0.016611870378255844, 0.10799410194158554, -0..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.read_parquet(\"/Users/doruktarhan/Desktop/LSR_Encoder_Results/flickr30k-blip-dense/img_embs.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated image error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted image: /Users/doruktarhan/Desktop/MSCOCO_Dataset/train2014/.DS_Store, Error: cannot identify image file '/Users/doruktarhan/Desktop/MSCOCO_Dataset/train2014/.DS_Store'\n",
      "Removed: /Users/doruktarhan/Desktop/MSCOCO_Dataset/train2014/.DS_Store\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def find_and_remove_corrupted_images(image_paths):\n",
    "    \"\"\"\n",
    "    Identify and delete corrupted images.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image file paths.\n",
    "    \"\"\"\n",
    "    corrupted_images = []\n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img.verify()  # Check if the file is an image\n",
    "        except (OSError, FileNotFoundError) as e:\n",
    "            print(f\"Corrupted image: {img_path}, Error: {e}\")\n",
    "            corrupted_images.append(img_path)\n",
    "\n",
    "    # Remove corrupted images\n",
    "    for img_path in corrupted_images:\n",
    "        try:\n",
    "            os.remove(img_path)\n",
    "            print(f\"Removed: {img_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting file {img_path}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "image_folder = \"/Users/doruktarhan/Desktop/MSCOCO_Dataset/train2014\"\n",
    "all_images = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder)]\n",
    "find_and_remove_corrupted_images(all_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
