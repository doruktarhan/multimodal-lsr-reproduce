{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1c4W5xjC2CT",
    "outputId": "d342c051-4eb8-431b-914f-ee6ca511607c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting salesforce-lavis\n",
      "  Using cached salesforce_lavis-1.0.2-py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: fairscale==0.4.4 in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.4.4)\n",
      "Requirement already satisfied: opencv-python-headless==4.5.5.64 in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (4.5.5.64)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.2.0)\n",
      "Requirement already satisfied: webdataset in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.2.100)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (2.2.3)\n",
      "Requirement already satisfied: streamlit in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (1.40.2)\n",
      "Requirement already satisfied: omegaconf in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (2.3.0)\n",
      "Requirement already satisfied: iopath in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.1.10)\n",
      "Requirement already satisfied: wheel in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.45.1)\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (3.8.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (4.67.1)\n",
      "Requirement already satisfied: decord in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.6.0)\n",
      "Requirement already satisfied: einops>=0.4.1 in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.8.0)\n",
      "Requirement already satisfied: ftfy in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (6.3.1)\n",
      "Requirement already satisfied: pycocoevalcap in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (1.2)\n",
      "Requirement already satisfied: python-magic in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.4.27)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (24.2)\n",
      "Requirement already satisfied: plotly in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (5.24.1)\n",
      "Requirement already satisfied: pre-commit in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (4.0.1)\n",
      "Requirement already satisfied: opendatasets in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.1.22)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.20.1)\n",
      "Requirement already satisfied: timm==0.4.12 in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.4.12)\n",
      "Requirement already satisfied: contexttimer in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.3.3)\n",
      "Requirement already satisfied: transformers<4.27,>=4.25.0 in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (4.26.1)\n",
      "Requirement already satisfied: pycocotools in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (2.0.8)\n",
      "Requirement already satisfied: scikit-image in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (0.24.0)\n",
      "Requirement already satisfied: ipython in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (8.18.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.venv/lib/python3.9/site-packages (from salesforce-lavis) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.9/site-packages (from opencv-python-headless==4.5.5.64->salesforce-lavis) (2.0.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.5.8)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (11.2.1.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (9.1.0.70)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (3.1.4)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (3.1.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (10.3.5.147)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.9/site-packages (from torch>=1.10.0->salesforce-lavis) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.10.0->salesforce-lavis) (1.3.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.9/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.venv/lib/python3.9/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (0.26.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (2024.11.6)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.9/site-packages (from ftfy->salesforce-lavis) (0.2.13)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.9/site-packages (from iopath->salesforce-lavis) (3.0.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.9/site-packages (from ipython->salesforce-lavis) (2.18.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.9/site-packages (from ipython->salesforce-lavis) (4.9.0)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.9/site-packages (from ipython->salesforce-lavis) (0.1.7)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.9/site-packages (from ipython->salesforce-lavis) (0.6.3)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.9/site-packages (from ipython->salesforce-lavis) (5.1.1)\n",
      "Requirement already satisfied: traitlets>=5 in ./.venv/lib/python3.9/site-packages (from ipython->salesforce-lavis) (5.14.3)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.9/site-packages (from ipython->salesforce-lavis) (3.0.48)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.9/site-packages (from ipython->salesforce-lavis) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.9/site-packages (from ipython->salesforce-lavis) (0.19.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.9/site-packages (from jedi>=0.16->ipython->salesforce-lavis) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.9/site-packages (from pexpect>4.3->ipython->salesforce-lavis) (0.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->salesforce-lavis) (3.0.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in ./.venv/lib/python3.9/site-packages (from omegaconf->salesforce-lavis) (4.9.3)\n",
      "Requirement already satisfied: kaggle in ./.venv/lib/python3.9/site-packages (from opendatasets->salesforce-lavis) (1.6.17)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.9/site-packages (from opendatasets->salesforce-lavis) (8.1.7)\n",
      "Requirement already satisfied: six>=1.10 in ./.venv/lib/python3.9/site-packages (from kaggle->opendatasets->salesforce-lavis) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in ./.venv/lib/python3.9/site-packages (from kaggle->opendatasets->salesforce-lavis) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in ./.venv/lib/python3.9/site-packages (from kaggle->opendatasets->salesforce-lavis) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in ./.venv/lib/python3.9/site-packages (from kaggle->opendatasets->salesforce-lavis) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in ./.venv/lib/python3.9/site-packages (from kaggle->opendatasets->salesforce-lavis) (2.2.3)\n",
      "Requirement already satisfied: bleach in ./.venv/lib/python3.9/site-packages (from kaggle->opendatasets->salesforce-lavis) (6.2.0)\n",
      "Requirement already satisfied: webencodings in ./.venv/lib/python3.9/site-packages (from bleach->kaggle->opendatasets->salesforce-lavis) (0.5.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas->salesforce-lavis) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas->salesforce-lavis) (2024.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./.venv/lib/python3.9/site-packages (from plotly->salesforce-lavis) (9.0.0)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in ./.venv/lib/python3.9/site-packages (from pre-commit->salesforce-lavis) (20.28.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in ./.venv/lib/python3.9/site-packages (from pre-commit->salesforce-lavis) (3.4.0)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in ./.venv/lib/python3.9/site-packages (from pre-commit->salesforce-lavis) (1.9.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in ./.venv/lib/python3.9/site-packages (from pre-commit->salesforce-lavis) (2.6.3)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in ./.venv/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->salesforce-lavis) (0.3.9)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in ./.venv/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->salesforce-lavis) (4.3.6)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in ./.venv/lib/python3.9/site-packages (from pycocotools->salesforce-lavis) (3.9.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (1.4.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (6.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (4.55.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (1.3.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (11.0.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools->salesforce-lavis) (3.21.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in ./.venv/lib/python3.9/site-packages (from python-slugify->kaggle->opendatasets->salesforce-lavis) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (3.4.0)\n",
      "Requirement already satisfied: imageio>=2.33 in ./.venv/lib/python3.9/site-packages (from scikit-image->salesforce-lavis) (2.36.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./.venv/lib/python3.9/site-packages (from scikit-image->salesforce-lavis) (2024.8.30)\n",
      "Requirement already satisfied: scipy>=1.9 in ./.venv/lib/python3.9/site-packages (from scikit-image->salesforce-lavis) (1.13.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./.venv/lib/python3.9/site-packages (from scikit-image->salesforce-lavis) (0.4)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (8.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (2.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (1.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (3.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (3.0.12)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (53.0.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (1.1.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (2.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (3.5.0)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (0.15.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (2.10.3)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (0.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.9/site-packages (from spacy->salesforce-lavis) (2.4.8)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->salesforce-lavis) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.venv/lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->salesforce-lavis) (1.2.1)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->salesforce-lavis) (2.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->salesforce-lavis) (0.7.0)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from thinc<8.4.0,>=8.3.0->spacy->salesforce-lavis) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.9/site-packages (from thinc<8.4.0,>=8.3.0->spacy->salesforce-lavis) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy->salesforce-lavis) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy->salesforce-lavis) (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->salesforce-lavis) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->salesforce-lavis) (0.1.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy->salesforce-lavis) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy->salesforce-lavis) (7.0.5)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.9/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->salesforce-lavis) (1.17.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.9/site-packages (from stack-data->ipython->salesforce-lavis) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.9/site-packages (from stack-data->ipython->salesforce-lavis) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.9/site-packages (from stack-data->ipython->salesforce-lavis) (0.2.3)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in ./.venv/lib/python3.9/site-packages (from streamlit->salesforce-lavis) (5.5.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in ./.venv/lib/python3.9/site-packages (from streamlit->salesforce-lavis) (18.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in ./.venv/lib/python3.9/site-packages (from streamlit->salesforce-lavis) (5.29.1)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in ./.venv/lib/python3.9/site-packages (from streamlit->salesforce-lavis) (6.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in ./.venv/lib/python3.9/site-packages (from streamlit->salesforce-lavis) (0.10.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in ./.venv/lib/python3.9/site-packages (from streamlit->salesforce-lavis) (5.5.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in ./.venv/lib/python3.9/site-packages (from streamlit->salesforce-lavis) (3.1.43)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from streamlit->salesforce-lavis) (1.9.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in ./.venv/lib/python3.9/site-packages (from streamlit->salesforce-lavis) (6.4.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in ./.venv/lib/python3.9/site-packages (from streamlit->salesforce-lavis) (0.9.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in ./.venv/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit->salesforce-lavis) (1.16.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in ./.venv/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit->salesforce-lavis) (4.23.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.9/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->salesforce-lavis) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->salesforce-lavis) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (0.22.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (24.2.0)\n",
      "Requirement already satisfied: braceexpand in ./.venv/lib/python3.9/site-packages (from webdataset->salesforce-lavis) (0.1.7)\n",
      "Installing collected packages: salesforce-lavis\n",
      "Successfully installed salesforce-lavis-1.0.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/gpfs/home2/scur2863/ondemand/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install salesforce-lavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seerL1QhAFeN",
    "outputId": "e8892a6f-c195-4681-93fc-2fb8ba4af4d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2863/ondemand/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/scur2863/ondemand/.venv/lib64/python3.9/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/home/scur2863/ondemand/.venv/lib64/python3.9/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from lavis.models import load_model_and_preprocess\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Architectures                  Types\n",
      "==================================================\n",
      "albef_classification           ve\n",
      "albef_feature_extractor        base\n",
      "albef_nlvr                     nlvr\n",
      "albef_pretrain                 base\n",
      "albef_retrieval                coco, flickr\n",
      "albef_vqa                      vqav2\n",
      "alpro_qa                       msrvtt, msvd\n",
      "alpro_retrieval                msrvtt, didemo\n",
      "blip_caption                   base_coco, large_coco\n",
      "blip_classification            base\n",
      "blip_feature_extractor         base\n",
      "blip_image_text_matching       base, large\n",
      "blip_nlvr                      nlvr\n",
      "blip_pretrain                  base\n",
      "blip_retrieval                 coco, flickr\n",
      "blip_vqa                       vqav2, okvqa, aokvqa\n",
      "blip2_opt                      pretrain_opt2.7b, pretrain_opt6.7b, caption_coco_opt2.7b, caption_coco_opt6.7b\n",
      "blip2_t5                       pretrain_flant5xl, pretrain_flant5xl_vitL, pretrain_flant5xxl, caption_coco_flant5xl\n",
      "blip2_feature_extractor        pretrain, pretrain_vitL, coco\n",
      "blip2                          pretrain, pretrain_vitL, coco\n",
      "blip2_image_text_matching      pretrain, pretrain_vitL, coco\n",
      "pnp_vqa                        base, large, 3b\n",
      "pnp_unifiedqav2_fid            \n",
      "img2prompt_vqa                 base\n",
      "clip_feature_extractor         ViT-B-32, ViT-B-16, ViT-L-14, ViT-L-14-336, RN50\n",
      "clip                           ViT-B-32, ViT-B-16, ViT-L-14, ViT-L-14-336, RN50\n",
      "gpt_dialogue                   base\n"
     ]
    }
   ],
   "source": [
    "from lavis.models import model_zoo\n",
    "print(model_zoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TextCaps dataset can be downloaded from https://textvqa.org/textcaps/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and preprocessors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess( name=\"albef_feature_extractor\", model_type=\"base\", is_eval=True, device=device)\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = \"dataset/\"\n",
    "\n",
    "# Path to JSON file containing captions\n",
    "caption_json_path = \"TextCaps_0.1_train.json\"\n",
    "\n",
    "# Batch size for processing\n",
    "batch_size = 16\n",
    "\n",
    "# Load captions JSON\n",
    "with open(caption_json_path, \"r\") as f:\n",
    "    caption_data = json.load(f)\n",
    "\n",
    "# Assert the structure of the JSON file\n",
    "assert \"data\" in caption_data, \"'data' key not found in JSON\"\n",
    "assert isinstance(caption_data[\"data\"], list), \"'data' key should contain a list of image dictionaries\"\n",
    "\n",
    "# Parse the captions and image IDs\n",
    "image_captions = {}\n",
    "for item in caption_data[\"data\"]:\n",
    "    assert \"image_id\" in item, \"'image_id' key not found in one of the image dictionaries\"\n",
    "    #assert \"reference_strs\" in item, \"'reference_strs' key not found in one of the image dictionaries\"\n",
    "    #assert isinstance(item[\"reference_strs\"], list) and len(item[\"reference_strs\"]) == 5, \\\n",
    "       # \"'reference_strs' must be a list containing exactly 5 captions\"\n",
    "\n",
    "    image_id = item[\"image_id\"]\n",
    "    captions = item[\"caption_str\"]\n",
    "    \n",
    "    image_captions[image_id] = captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dump_parquet(images_ids,image_embeddings):\n",
    "  image_embeddings = torch.cat(image_embeddings, dim=0).cpu()\n",
    "  embeddings_list = image_embeddings.tolist()\n",
    "\n",
    "  df_ = pd.DataFrame({\n",
    "    \"id\": images_ids,\n",
    "\n",
    "    \"embedding\": embeddings_list})\n",
    "  output_path = 'image_embeddings_albef_new2.parquet'\n",
    "\n",
    "  # Save as Parquet\n",
    "  df_.to_parquet(output_path, engine='pyarrow')\n",
    "  print(f\"Embeddings successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BcUGw4TcsjsE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "54106dbe-88b9-4811-d672-1b953554717b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000\n",
      "Embeddings successfully saved to image_embeddings_albef_new2.parquet\n",
      "20000\n",
      "Embeddings successfully saved to image_embeddings_albef_new2.parquet\n",
      "21000\n",
      "Embeddings successfully saved to image_embeddings_albef_new2.parquet\n",
      "Embeddings successfully saved to image_embeddings_albef_new2.parquet\n",
      "Total Images Processed: 21953\n",
      "Image Embedding Shape: torch.Size([2952, 256])\n"
     ]
    }
   ],
   "source": [
    "# Helper function to process a batch of images and captions\n",
    "batch_size = 4\n",
    "import numpy as np\n",
    "np.Inf = np.inf\n",
    "def process_batch(image_paths):\n",
    "    processed_images = []\n",
    "    #processed_texts = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        raw_image = Image.open(path).convert(\"RGB\")  # Ensure all images are RGB\n",
    "        processed_images.append(vis_processors[\"eval\"](raw_image).unsqueeze(0))  # Process image\n",
    "        #processed_texts.append([txt_processors[\"eval\"](caption) for caption in captions])  # Process each caption\n",
    "\n",
    "    # Stack processed images into a single tensor\n",
    "    batched_images = torch.cat(processed_images, dim=0).to(device)\n",
    "    \n",
    "    textt=[]\n",
    "    for k in range(len(batched_images)):\n",
    "        textt.append('rr r rr')\n",
    "    \n",
    "    # Flatten captions into a single list for this batch\n",
    "    #flat_texts = [item for sublist in processed_texts for item in sublist]\n",
    "\n",
    "    # Create a sample for the model\n",
    "    sample = {\"image_id\":torch.tensor([k+1 for k in range(len(batched_images))]).to(device),\"image\": batched_images, \"text_input\": textt, \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 100}\n",
    "    return sample\n",
    "\n",
    "# Get all image paths and verify matching captions\n",
    "all_image_paths = []\n",
    "#all_captions = []\n",
    "\n",
    "for image_id, captions in image_captions.items():\n",
    "    image_path = os.path.join(image_dir, image_id +'.jpg')\n",
    "    assert os.path.exists(image_path), f\"Image file not found: {image_path}\"\n",
    "    all_image_paths.append(image_path)\n",
    "    #all_captions.append(captions)\n",
    "\n",
    "# Process images and captions in batches\n",
    "image_embeddings = []\n",
    "image_ids=[]\n",
    "#text_embeddings = []\n",
    "#similarity_matrices = []\n",
    "\n",
    "for i in range(19000, len(all_image_paths)-1, batch_size):\n",
    "    # Get batch of image paths and captions\n",
    "    batch_paths = all_image_paths[i:i+batch_size]\n",
    "    #batch_captions = all_captions[i:i+batch_size]\n",
    "\n",
    "    # Prepare the sample\n",
    "    sample = process_batch(batch_paths)\n",
    "    \n",
    "\n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        output = model(sample)\n",
    "    features_text = model.vision_proj(output.intermediate_output.image_embeds[:,0,:])\n",
    "    # Append embeddings\n",
    "    image_embeddings.append(features_text)\n",
    "    \n",
    "    #text_embeddings.append(features_text.text_embeds_proj)\n",
    "    image_ids.extend(list(image_captions.keys())[i:i+batch_size])\n",
    "    \n",
    "    # Compute similarity between image and captions (averaging over captions per image)\n",
    "    '''\n",
    "    for idx in range(len(batch_paths)):\n",
    "        image_feature = features_image.image_embeds_proj[idx, 0, :]\n",
    "        text_features = features_text.text_embeds_proj[idx*5:(idx+1)*5, 0, :]  # 5 captions per image\n",
    "        similarity = torch.mean(image_feature @ text_features.t())\n",
    "        similarity_matrices.append(similarity)\n",
    "      '''\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "        dump_parquet(image_ids,image_embeddings)\n",
    "\n",
    "\n",
    "# Stack all batches together\n",
    "dump_parquet(image_ids,image_embeddings)\n",
    "image_embeddings = torch.cat(image_embeddings, dim=0)  # [Total Images, Seq Length, Embedding Dim]\n",
    "#text_embeddings = torch.cat(text_embeddings, dim=0)    # [Total Captions, Seq Length, Embedding Dim]\n",
    "\n",
    "# Final similarity matrix (aggregated)\n",
    "#similarity_matrix = torch.stack(similarity_matrices)\n",
    "\n",
    "# Output results\n",
    "print(\"Total Images Processed:\", len(all_image_paths))\n",
    "print(\"Image Embedding Shape:\", image_embeddings.shape)\n",
    "#print(\"Text Embedding Shape:\", text_embeddings.shape)\n",
    "#print(\"Similarity Matrix Shape:\", similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7A3mf3uUyIE",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKYLXLaW0vxi"
   },
   "source": [
    "## Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H-fvQBsHLXhk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dump_parquet_text(senten_ids,text_embeddings):\n",
    "  text_embeddings = torch.cat(text_embeddings, dim=0).to(device)\n",
    "  embeddings_list = text_embeddings.tolist()\n",
    "  df_ = pd.DataFrame({\n",
    "    \"id\": senten_ids,\n",
    "    \"embedding\": embeddings_list}\n",
    "                     )\n",
    "  output_path = 'text_embeddings_albef_new2.parquet'\n",
    "  #print(df_)\n",
    "  # Save as Parquet\n",
    "  df_.to_parquet(output_path, engine='pyarrow')\n",
    "  print(f\"Embeddings successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qNXSS6h8H8lh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `invalid multinomial distribution (sum of probabilities <= 0)` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#features_text = model2.extract_features(sample, mode=\"text\")\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 61\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m features_text \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtext_proj(output\u001b[38;5;241m.\u001b[39mintermediate_output\u001b[38;5;241m.\u001b[39mtext_embeds[:,\u001b[38;5;241m0\u001b[39m,:])\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Append embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/ondemand/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ondemand/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ondemand/.venv/lib64/python3.9/site-packages/lavis/models/albef_models/albef_retrieval.py:241\u001b[0m, in \u001b[0;36mAlbefRetrieval.forward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    239\u001b[0m image_embeds_neg \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(bs):\n\u001b[0;32m--> 241\u001b[0m     neg_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_t2i\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    242\u001b[0m     image_embeds_neg\u001b[38;5;241m.\u001b[39mappend(image_embeds[neg_idx])\n\u001b[1;32m    243\u001b[0m image_embeds_neg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(image_embeds_neg, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.Inf = np.inf\n",
    "def process_batch_text(captions_list):\n",
    "    processed_images = []\n",
    "    processed_texts = []\n",
    "\n",
    "    for caption in  captions_list:\n",
    "        \n",
    "        #raw_image = Image.open(path).convert(\"RGB\")  # Ensure all images are RGB\n",
    "        #processed_images.append(vis_processors[\"eval\"](raw_image).unsqueeze(0))  # Process image\n",
    "        processed_texts.append(txt_processors[\"eval\"](caption))  # Process each caption\n",
    "        #processed_images.append(torch.rand(16,224,224))\n",
    "    # Stack processed images into a single tensor\n",
    "    \n",
    "    batched_images = torch.cat([torch.zeros(1,3,384,384)]*len(processed_texts), dim=0).to(device)\n",
    "    #batched_text = [item for sublist in processed_texts]\n",
    "    # Flatten captions into a single list for this batch\n",
    "    assert len(processed_texts) == len(batched_images) , f\"mismatch{len(processed_texts)}!={len(batched_images) }\"\n",
    "     #[item for sublist in processed_texts]\n",
    "    #batched_images=torch.zeros(5,3,224,224)\n",
    "    # Create a sample for the model\n",
    "    sample = {\"image_id\":torch.tensor([k+1 for k in range(len(batched_images))]).to(device),\"image\": batched_images, \"text_input\": processed_texts, \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 100}\n",
    "    return sample\n",
    "\n",
    "# Get all image paths and verify matching captions\n",
    "all_image_paths = []\n",
    "all_captions = []\n",
    "\n",
    "# Parse the captions and image IDs\n",
    "image_captions_full = {}\n",
    "for item in caption_data[\"data\"]:\n",
    "    caption_id = item[\"caption_id\"]\n",
    "    captions = item[\"caption_str\"]\n",
    "    image = item[\"image_id\"]\n",
    "    image_captions_full[caption_id] = {'text':captions, 'image':image}\n",
    "\n",
    "\n",
    "for item in image_captions_full.values():\n",
    "    image_id =item['image']\n",
    "    caption = item['text']\n",
    "    #image_path = os.path.join(image_dir, image_id +'.jpg')\n",
    "    #assert os.path.exists(image_path), f\"Image file not found: {image_path}\"\n",
    "    #all_image_paths.append(image_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Process images and captions in batches\n",
    "image_id = []\n",
    "text_embedding = []\n",
    "sentence_id=[]\n",
    "\n",
    "for i in range(, len(all_captions), batch_size):\n",
    "\n",
    "    # Get batch of image paths and captions\n",
    "    batch_captions = all_captions[i:i+batch_size]\n",
    "    #print(batch_captions)\n",
    "\n",
    "    # Prepare the sample\n",
    "    sample = process_batch_text(batch_captions)\n",
    "\n",
    "    assert not torch.isnan(sample[\"image\"]).any()\n",
    "    assert not torch.isinf(sample[\"image\"]).any()\n",
    "    # Extract features\n",
    "    #features_text = model2.extract_features(sample, mode=\"text\")\n",
    "    with torch.no_grad():\n",
    "        output = model(sample)\n",
    "    \n",
    "    features_text = model.text_proj(output.intermediate_output.text_embeds[:,0,:])\n",
    "    # Append embeddings\n",
    "    text_embedding.append(features_text)\n",
    "    \n",
    "    sentence_id.extend(list(image_captions_full.keys())[i:i+batch_size])\n",
    "    \n",
    "    # Compute similarity between image and captions (averaging over captions per image)\n",
    "    '''\n",
    "    for idx in range(len(batch_paths)):\n",
    "        image_feature = features_image.image_embeds_proj[idx, 0, :]\n",
    "        text_features = features_text.text_embeds_proj[idx*5:(idx+1)*5, 0, :]  # 5 captions per image\n",
    "        similarity = torch.mean(image_feature @ text_features.t())\n",
    "        similarity_matrices.append(similarity)\n",
    "      '''\n",
    "    if i%5001==0:\n",
    "      print(i)\n",
    "      dump_parquet_text(sentence_id,text_embedding)\n",
    "\n",
    "\n",
    "# Stack all batches together\n",
    "dump_parquet_text(sentence_id,text_embedding)\n",
    "text_embeddings = torch.cat(text_embeddings, dim=0)    # [Total Captions, Seq Length, Embedding Dim]\n",
    "\n",
    "# Final similarity matrix (aggregated)\n",
    "#similarity_matrix = torch.stack(similarity_matrices)\n",
    "\n",
    "# Output results\n",
    "#print(\"Total Images Processed:\", len(all_image_paths))\n",
    "print(\"Text Embedding Shape:\", text_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#features_text = model2.extract_features(sample, mode=\"text\")\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 57\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m features_text \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mtext_proj(output\u001b[38;5;241m.\u001b[39mintermediate_output\u001b[38;5;241m.\u001b[39mtext_embeds[:,\u001b[38;5;241m0\u001b[39m,:])\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Append embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/ondemand/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ondemand/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ondemand/.venv/lib64/python3.9/site-packages/lavis/models/albef_models/albef_pretrain.py:238\u001b[0m, in \u001b[0;36mAlbefPretrain.forward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    236\u001b[0m image_embeds_neg \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(bs):\n\u001b[0;32m--> 238\u001b[0m     neg_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_t2i\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    239\u001b[0m     image_embeds_neg\u001b[38;5;241m.\u001b[39mappend(image_embeds[neg_idx])\n\u001b[1;32m    240\u001b[0m image_embeds_neg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(image_embeds_neg, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "batch_size=4\n",
    "import numpy as np\n",
    "np.Inf = np.inf\n",
    "def process_batch_text(captions_list):\n",
    "    processed_images = []\n",
    "    processed_texts = []\n",
    "\n",
    "    for caption in  captions_list:\n",
    "        \n",
    "        #raw_image = Image.open(path).convert(\"RGB\")  # Ensure all images are RGB\n",
    "        #processed_images.append(vis_processors[\"eval\"](raw_image).unsqueeze(0))  # Process image\n",
    "        processed_texts.append(txt_processors[\"eval\"](caption))  # Process each caption\n",
    "        #processed_images.append(torch.rand(16,224,224))\n",
    "    # Stack processed images into a single tensor\n",
    "    \n",
    "    batched_images = torch.cat([torch.zeros(1,3,224,224)]*len(processed_texts), dim=0).to(device)\n",
    "    #batched_text = [item for sublist in processed_texts]\n",
    "    # Flatten captions into a single list for this batch\n",
    "    assert len(processed_texts) == len(batched_images) , f\"mismatch{len(processed_texts)}!={len(batched_images) }\"\n",
    "     #[item for sublist in processed_texts]\n",
    "    #batched_images=torch.zeros(5,3,224,224)\n",
    "    # Create a sample for the model\n",
    "    sample = {\"image\": torch.zeros(1,3,224,224), \"text_input\": ['A black and white graphic drawing without text on it'], \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 100}\n",
    "    return sample\n",
    "\n",
    "# Get all image paths and verify matching captions\n",
    "all_image_paths = []\n",
    "all_captions = []\n",
    "\n",
    "\n",
    "for item in image_captions_full.values():\n",
    "    image_id =item['image']\n",
    "    caption = item['text']\n",
    "    #image_path = os.path.join(image_dir, image_id +'.jpg')\n",
    "    #assert os.path.exists(image_path), f\"Image file not found: {image_path}\"\n",
    "    #all_image_paths.append(image_path)\n",
    "    all_captions.append((caption)\n",
    "\n",
    "# Process images and captions in batches\n",
    "image_id = []\n",
    "text_embedding = []\n",
    "sentence_id=[]\n",
    "for i in range(1):\n",
    "\n",
    "    # Get batch of image paths and captions\n",
    "    batch_captions = ['100109764']\n",
    "    #print(batch_captions)\n",
    "\n",
    "    # Prepare the sample\n",
    "    sample = process_batch_text(batch_captions)\n",
    "\n",
    "    assert not torch.isnan(sample[\"image\"]).any()\n",
    "    assert not torch.isinf(sample[\"image\"]).any()\n",
    "    # Extract features\n",
    "    #features_text = model2.extract_features(sample, mode=\"text\")\n",
    "    with torch.no_grad():\n",
    "        output = model2(sample)\n",
    "    \n",
    "    features_text = model2.text_proj(output.intermediate_output.text_embeds[:,0,:])\n",
    "    # Append embeddings\n",
    "    text_embedding.append(features_text)\n",
    "    \n",
    "    sentence_id.extend(list(image_captions_full.keys())[i:i+batch_size])\n",
    "    \n",
    "    # Compute similarity between image and captions (averaging over captions per image)\n",
    "    '''\n",
    "    for idx in range(len(batch_paths)):\n",
    "        image_feature = features_image.image_embeds_proj[idx, 0, :]\n",
    "        text_features = features_text.text_embeds_proj[idx*5:(idx+1)*5, 0, :]  # 5 captions per image\n",
    "        similarity = torch.mean(image_feature @ text_features.t())\n",
    "        similarity_matrices.append(similarity)\n",
    "      '''\n",
    "    if i%5001==0:\n",
    "      print(i)\n",
    "      dump_parquet_text(sentence_id,text_embedding)\n",
    "\n",
    "\n",
    "# Stack all batches together\n",
    "dump_parquet_text(sentence_id,text_embedding)\n",
    "text_embeddings = torch.cat(text_embeddings, dim=0)    # [Total Captions, Seq Length, Embedding Dim]\n",
    "\n",
    "# Final similarity matrix (aggregated)\n",
    "#similarity_matrix = torch.stack(similarity_matrices)\n",
    "\n",
    "# Output results\n",
    "#print(\"Total Images Processed:\", len(all_image_paths))\n",
    "print(\"Text Embedding Shape:\", text_embeddings.shape)\n",
    "#print(\"Text Embedding Shape:\", text_embeddings.shape)\n",
    "#print(\"Similarity Matrix Shape:\", similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "# Path to JSON file containing captions\n",
    "caption_json_path = \"TextCaps_0.1_train.json\"\n",
    "\n",
    "# Batch size for processing\n",
    "batch_size = 16\n",
    "\n",
    "# Load captions JSON\n",
    "with open(caption_json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "# Extract unique picture IDs\n",
    "unique_picture_ids = {item[\"image_id\"] for item in data['data']}\n",
    "\n",
    "# Shuffle unique picture IDs\n",
    "unique_picture_ids = list(unique_picture_ids)\n",
    "random.shuffle(unique_picture_ids)\n",
    "\n",
    "# Split picture IDs into train, val, and test\n",
    "num_pictures = len(unique_picture_ids)\n",
    "train_split = int(0.6 * num_pictures)\n",
    "val_split = int(0.2 * num_pictures)\n",
    "\n",
    "train_ids = set(unique_picture_ids[:train_split])\n",
    "val_ids = set(unique_picture_ids[train_split:train_split + val_split])\n",
    "test_ids = set(unique_picture_ids[train_split + val_split:])\n",
    "\n",
    "# Add 'type' label to each item in the data\n",
    "for item in data['data']:\n",
    "    if item[\"image_id\"] in train_ids:\n",
    "        item[\"type\"] = \"train\"\n",
    "    elif item[\"image_id\"] in val_ids:\n",
    "        item[\"type\"] = \"val\"\n",
    "    elif item[\"image_id\"] in test_ids:\n",
    "        item[\"type\"] = \"test\"\n",
    "\n",
    "# Save updated JSON\n",
    "\n",
    "with open(\"metadata_split.json\", \"w\") as f:\n",
    "    json.dump(data, f, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "print(\"Data split into train, val, and test sets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2863/ondemand/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_path = 'image_embeddings_albef_new.parquet'\n",
    "\n",
    "image_df = pd.read_parquet(output_path, engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'text_embeddings_albef_new.parquet'\n",
    "text_df = pd.read_parquet(output_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000000</td>\n",
       "      <td>[-0.3620331287384033, -0.16910064220428467, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100000001</td>\n",
       "      <td>[0.4743715822696686, 1.591766595840454, -1.348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000002</td>\n",
       "      <td>[0.14031903445720673, 0.3389507234096527, -1.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100000003</td>\n",
       "      <td>[-0.036332644522190094, 0.2871374189853668, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100000004</td>\n",
       "      <td>[0.053886838257312775, 0.24197791516780853, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109760</th>\n",
       "      <td>100109760</td>\n",
       "      <td>[0.629140317440033, 0.41149383783340454, 0.514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109761</th>\n",
       "      <td>100109761</td>\n",
       "      <td>[0.629140317440033, 0.41149383783340454, 0.514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109762</th>\n",
       "      <td>100109762</td>\n",
       "      <td>[0.629140317440033, 0.41149383783340454, 0.514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109763</th>\n",
       "      <td>100109763</td>\n",
       "      <td>[0.629140317440033, 0.41149383783340454, 0.514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109764</th>\n",
       "      <td>100109764</td>\n",
       "      <td>[0.629140317440033, 0.41149383783340454, 0.514...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109765 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                          embedding\n",
       "0       100000000  [-0.3620331287384033, -0.16910064220428467, -1...\n",
       "1       100000001  [0.4743715822696686, 1.591766595840454, -1.348...\n",
       "2       100000002  [0.14031903445720673, 0.3389507234096527, -1.4...\n",
       "3       100000003  [-0.036332644522190094, 0.2871374189853668, -1...\n",
       "4       100000004  [0.053886838257312775, 0.24197791516780853, -1...\n",
       "...           ...                                                ...\n",
       "109760  100109760  [0.629140317440033, 0.41149383783340454, 0.514...\n",
       "109761  100109761  [0.629140317440033, 0.41149383783340454, 0.514...\n",
       "109762  100109762  [0.629140317440033, 0.41149383783340454, 0.514...\n",
       "109763  100109763  [0.629140317440033, 0.41149383783340454, 0.514...\n",
       "109764  100109764  [0.629140317440033, 0.41149383783340454, 0.514...\n",
       "\n",
       "[109765 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the captions and image IDs\n",
    "image_captions_full = {}\n",
    "for item in caption_data[\"data\"]:\n",
    "    caption_id = item[\"caption_id\"]\n",
    "    captions = item[\"caption_str\"]\n",
    "    image = item[\"image_id\"]\n",
    "    image_captions_full[caption_id] = {'text':caption_id, 'image':image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities=[]\n",
    "for item in list(image_captions_full.values()):\n",
    "    \n",
    "    image_id =item['image']\n",
    "    caption = item['text']\n",
    "    \n",
    "    text_emb=torch.tensor(text_df[text_df['id']==caption]['embedding'].values[0])\n",
    "    \n",
    "    \n",
    "    image_emb=torch.tensor(image_df[image_df['id']==image_id]['embedding'].values[0])\n",
    "    #print(np.shape(image_emb.reshape(( 1,256))))\n",
    "    # Normalize embeddings to unit vectors\n",
    "    text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
    "    image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = image_emb @ text_emb.t()\n",
    "    similarities.append(similarity.item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0915124160735395)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hf_hub_download' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dense_embs \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malvarodelamaza/textcaps-blip-dense\u001b[39m\u001b[38;5;124m'\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_emb\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_embeddings_blip.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_emb\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embeddings_blip.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m}, keep_in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mwith_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m meta_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[43mhf_hub_download\u001b[49m(\n\u001b[1;32m      4\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata, repo_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata_split.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      5\u001b[0m text_ids \u001b[38;5;241m=\u001b[39m dense_embs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_emb\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m text_embs \u001b[38;5;241m=\u001b[39m dense_embs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_emb\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memb\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hf_hub_download' is not defined"
     ]
    }
   ],
   "source": [
    "dense_embs = load_dataset('alvarodelamaza/textcaps-blip-dense', data_files={\"img_emb\": \"image_embeddings_blip.parquet\",\n",
    "                                                        \"text_emb\": \"text_embeddings_blip.parquet\"}, keep_in_memory=True).with_format(\"numpy\")\n",
    "text_ids = dense_embs['text_emb'][\"id\"]\n",
    "text_embs = dense_embs['text_emb']['emb']\n",
    "img_ids = dense_embs['img_emb']['id']\n",
    "img_embs = dense_embs['img_emb']['emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/ondemand/.venv/lib64/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtext_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mcaption\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/ondemand/.venv/lib64/python3.9/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/ondemand/.venv/lib64/python3.9/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/ondemand/.venv/lib64/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "len(text_df[text_df['id']==caption]['embedding'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aOydpFe31ks2",
    "EXQFrzh0lwEL",
    "GKYLXLaW0vxi"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18450b34054e4012a796be3c60cb06f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f96c1e56a8354bf5bd2cb728b52b236f",
      "placeholder": "​",
      "style": "IPY_MODEL_7dc97ce3fc2c481d8986eb67b52dcf45",
      "value": " 440M/440M [00:08&lt;00:00, 123MB/s]"
     }
    },
    "7dc97ce3fc2c481d8986eb67b52dcf45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c62f2bd36a9a40a1a0f41b25902f6e23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e89557cddfa24f809645dde61fd9dfae",
      "placeholder": "​",
      "style": "IPY_MODEL_f1a1e2554f1741c79b9fc53ba0632d77",
      "value": "model.safetensors: 100%"
     }
    },
    "d99dbd8ef45d40f08b6e10813b42e5ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da88ff9175404182855512aeb728b6be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c62f2bd36a9a40a1a0f41b25902f6e23",
       "IPY_MODEL_f0685844ef764394b22a20fed4516e5f",
       "IPY_MODEL_18450b34054e4012a796be3c60cb06f7"
      ],
      "layout": "IPY_MODEL_fdc815ea66c14d85b9d51980e5f5885f"
     }
    },
    "e89557cddfa24f809645dde61fd9dfae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9781bfd9d124d98afd3762c3121b5e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f0685844ef764394b22a20fed4516e5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d99dbd8ef45d40f08b6e10813b42e5ee",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9781bfd9d124d98afd3762c3121b5e9",
      "value": 440449768
     }
    },
    "f1a1e2554f1741c79b9fc53ba0632d77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f96c1e56a8354bf5bd2cb728b52b236f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdc815ea66c14d85b9d51980e5f5885f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
