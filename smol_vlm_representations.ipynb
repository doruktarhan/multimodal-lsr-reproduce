{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForImageTextRetrieval(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): BlipTextModel(\n",
       "    (embeddings): BlipTextEmbeddings(\n",
       "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): BlipTextEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BlipTextLayer(\n",
       "          (attention): BlipTextAttention(\n",
       "            (self): BlipTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BlipTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BlipTextAttention(\n",
       "            (self): BlipTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BlipTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BlipTextIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BlipTextOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vision_proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (text_proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (itm_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, BlipForImageTextRetrieval\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Load the model and processor\n",
    "#model_name = \"HuggingFaceTB/SmolVLM-Base\" #model card for image text matching\n",
    "model_name = \"Salesforce/blip-itm-base-coco\" #model card for image text matching\n",
    "\n",
    "\n",
    "model = BlipForImageTextRetrieval.from_pretrained(model_name)\n",
    "model.config.with_projection = True #get the projections for the image and text\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   The [`BlipForImageTextRetrieval`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "    <Tip>\n",
      "\n",
      "    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "    the latter silently ignores them.\n",
      "\n",
      "    </Tip>\n",
      "\n",
      "    Args:\n",
      "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
      "            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n",
      "            [`BlipImageProcessor`]. See [`BlipImageProcessor.__call__`] for details.\n",
      "        output_attentions (`bool`, *optional*):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "            tensors for more detail.\n",
      "        output_hidden_states (`bool`, *optional*):\n",
      "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "            more detail.\n",
      "        return_dict (`bool`, *optional*):\n",
      "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to interpolate the pre-trained position encodings.\n",
      "\n",
      "\n",
      "        Returns:\n",
      "            [`transformers.models.blip.modeling_blip.BlipTextVisionModelOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.models.blip.modeling_blip.BlipTextVisionModelOutput`] or a tuple of\n",
      "            `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "            elements depending on the configuration ([`<class 'transformers.models.blip.configuration_blip.BlipVisionConfig'>`]) and inputs.\n",
      "\n",
      "            - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Languge modeling loss from the text decoder.\n",
      "            - **image_embeds** (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`) -- The image embeddings obtained by applying the projection layer to the pooler_output.\n",
      "            - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-states at the output of the last layer of the model.\n",
      "            - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "              one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "              Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "            - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "              sequence_length)`.\n",
      "\n",
      "              Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "              heads.\n",
      "      \n",
      "\n",
      "        Examples:\n",
      "\n",
      "        ```python\n",
      "        >>> from PIL import Image\n",
      "        >>> import requests\n",
      "        >>> from transformers import AutoProcessor, BlipForImageTextRetrieval\n",
      "\n",
      "        >>> model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
      "        >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
      "\n",
      "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
      "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
      "        >>> text = \"an image of a cat\"\n",
      "\n",
      "        >>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
      "        >>> outputs = model(**inputs)\n",
      "        ```\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(model.forward.__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.vision_config.with_projection = True\n",
    "model.config.text_config.with_projection = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"Salesforce/blip-itm-base-coco\",\n",
       "  \"architectures\": [\n",
       "    \"BlipForImageTextRetrieval\"\n",
       "  ],\n",
       "  \"image_text_hidden_size\": 256,\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label_smoothing\": 0.0,\n",
       "  \"logit_scale_init_value\": 2.6592,\n",
       "  \"model_type\": \"blip\",\n",
       "  \"projection_dim\": 512,\n",
       "  \"text_config\": {\n",
       "    \"_attn_implementation_autoset\": true,\n",
       "    \"initializer_factor\": 1.0,\n",
       "    \"model_type\": \"blip_text_model\",\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"with_projection\": true\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"vision_config\": {\n",
       "    \"_attn_implementation_autoset\": true,\n",
       "    \"dropout\": 0.0,\n",
       "    \"initializer_factor\": 1.0,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"model_type\": \"blip_vision_model\",\n",
       "    \"num_channels\": 3,\n",
       "    \"with_projection\": true\n",
       "  },\n",
       "  \"with_projection\": true\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filepath', 'sentids', 'filename', 'imgid', 'split', 'sentences', 'cocoid'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create batches\n",
    "#load metadata\n",
    "meta_data_path = 'data/dataset_coco.json'\n",
    "with open(meta_data_path, 'r') as f:\n",
    "    meta_data = json.load(f)\n",
    "\n",
    "\n",
    "#image path \n",
    "path_to_dataset_folder = '/Users/doruktarhan/Desktop/MSCOCO_Dataset' #dataset images folder path\n",
    "\n",
    "meta_data['images'][0].keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images and captions: 100%|██████████| 123287/123287 [00:25<00:00, 4890.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image-caption pairs: 616767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "image_caption_pairs = []\n",
    "\n",
    "for image_data in tqdm(meta_data['images'], desc=\"Processing images and captions\"):\n",
    "    #save the image id\n",
    "    image_id = image_data['imgid']\n",
    "\n",
    "    #save the image path\n",
    "    image_folder = image_data['filepath']\n",
    "    image_name = image_data['filename']\n",
    "    image_path = os.path.join(path_to_dataset_folder, image_folder)\n",
    "    image_path = os.path.join(image_path, image_name)\n",
    "\n",
    "    #get the image\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    for sentence_meta_data in image_data['sentences']:\n",
    "        #get the sentence_id\n",
    "        sentence_id = sentence_meta_data['sentid']\n",
    "        #get the sentence\n",
    "        caption = sentence_meta_data['raw']\n",
    "        image_caption_pairs.append((image_id, sentence_id, image_path, caption))\n",
    "        \n",
    "\n",
    "    \n",
    "print(f\"Total image-caption pairs: {len(image_caption_pairs)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import normalize\n",
    "\n",
    "\n",
    "imgid,sentid, image_path, caption =  image_caption_pairs[0]\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "inputs = processor(images=image, text=caption, return_tensors=\"pt\").to(device)\n",
    "\n",
    "inputs.keys()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 384, 384]), torch.Size([1, 18]), torch.Size([1, 18]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'].shape, inputs['input_ids'].shape, inputs['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image projection shape without projection head: torch.Size([1, 577, 768])\n",
      "Text projection shape without projection head: torch.Size([1, 18, 768])\n",
      "Image projection shape: torch.Size([1, 256])\n",
      "Text projection shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, use_itm_head=False,return_dict=True)\n",
    "    proj_text_embedding = normalize(model.text_proj(outputs.question_embeds[:,0,:]))\n",
    "    proj_image_embedding = normalize(model.vision_proj(outputs.last_hidden_state[:,0,:]))\n",
    "\n",
    "print(f\"Image projection shape without projection head: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"Text projection shape without projection head: {outputs.question_embeds.shape}\")\n",
    "\n",
    "print(f\"Image projection shape: {proj_image_embedding.shape}\")\n",
    "print(f\"Text projection shape: {proj_text_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man with a red helmet on a small moped on a dirt road. \n"
     ]
    }
   ],
   "source": [
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise cosine similarity:\n",
      "tensor([[0.4718]])\n"
     ]
    }
   ],
   "source": [
    "# Compute pairwise cosine similarity (matrix)\n",
    "pairwise_cosine_sim = torch.matmul(proj_image_embedding, proj_text_embedding.T)\n",
    "\n",
    "print(f\"Pairwise cosine similarity:\\n{pairwise_cosine_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise cosine similarity:\n",
      "tensor([[-0.0889]])\n"
     ]
    }
   ],
   "source": [
    "pairwise_cosine_sim = torch.matmul(outputs.question_embeds[:,0,:], outputs.last_hidden_state[:,0,:].T)\n",
    "\n",
    "print(f\"Pairwise cosine similarity:\\n{pairwise_cosine_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())  # Should print True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
